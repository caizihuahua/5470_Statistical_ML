{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "463f2485",
   "metadata": {},
   "source": [
    "# Replication of Gu, Kelly and Xiu (2020, RFS)\n",
    "\n",
    "Current Version: Jan 2023\n",
    "\n",
    "By [Yanlin Bao](https://www.ylbao.dev/), PhD in Business (Finance) student at Singapore Management University.\n",
    "\n",
    "email: ylbao.2022@pbs.smu.edu.sg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b58316",
   "metadata": {},
   "source": [
    "This jupyter notebook is a simple version of replication code in Python of the seminal work [Gu, Kelly and Xiu (2020)](https://doi.org/10.1093/rfs/hhaa009). In this version, I mainly focus on the implementation of the models used in this paper. I hope this can be a reference for researchers, students and practitioners who want to explore the machine learning application in asset pricing. I run this version on my laptop. Therefore, I only present the result of a subsample of the original dataset, and only focus on the top 1000 firm with respect to market capitalization. I also tried for the whole firm space and the bottom 1000 firms. The results are consistent with those in the paper that the stock return of large firms are more predictable. As a result, I tried some more agressive hyperparameter setting in validation process.\n",
    "\n",
    "Apart from the original RFS paper, for more details about the implementation, I refer the readers to the [online appendix](https://dachxiu.chicagobooth.edu/download/ML_supp.pdf) and [Q&A](https://www.dropbox.com/s/4vsc4hakwvz2j31/ML_QandA.pdf?dl=0) provided by the authors. Since the machine learning models have too many hyperparameters to choose, and the training process of some models have random initial guess for the parameters, fully replication of the results from the paper is impossible.\n",
    "\n",
    "Besides, I believe there are some mistakes in my code, and there are still some unsolved problems. Please feel free to play the code and contact me through email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f09b44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gc # garbage collection module to release memory usage in time\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# os.chdir('/Users/baoyanlin/Replication_Code/GKX_2020_RFS/data')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e519bd7c",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "### Firm Characteristics Data\n",
    "\n",
    "The newest version of firm characteristics data are downloaded from Prof Dacheng Xiu's [website](https://dachxiu.chicagobooth.edu/). The data may take 4 to 6 hours to download though. Prof [Yuan Yao](https://yao-lab.github.io/) provide a dropbox [link](https://www.dropbox.com/s/zzgjdubvv23xkfp/datashare.zip?dl=0) which takes only a few minutes to download. However, the current version ends at 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff6c9f30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.3 s, sys: 2.38 s, total: 25.7 s\n",
      "Wall time: 25.7 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>DATE</th>\n",
       "      <th>mvel1</th>\n",
       "      <th>RET</th>\n",
       "      <th>prc</th>\n",
       "      <th>SHROUT</th>\n",
       "      <th>beta</th>\n",
       "      <th>betasq</th>\n",
       "      <th>chmom</th>\n",
       "      <th>dolvol</th>\n",
       "      <th>...</th>\n",
       "      <th>baspread</th>\n",
       "      <th>ill</th>\n",
       "      <th>maxret</th>\n",
       "      <th>retvol</th>\n",
       "      <th>std_dolvol</th>\n",
       "      <th>std_turn</th>\n",
       "      <th>zerotrade</th>\n",
       "      <th>sic2</th>\n",
       "      <th>bm</th>\n",
       "      <th>bm_ia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>24355.500</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>9.8750</td>\n",
       "      <td>2498</td>\n",
       "      <td>0.037079</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.281788</td>\n",
       "      <td>8.395576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020711</td>\n",
       "      <td>1.098587e-06</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.017710</td>\n",
       "      <td>0.972710</td>\n",
       "      <td>0.426715</td>\n",
       "      <td>4.200000e+00</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.868139</td>\n",
       "      <td>-0.104925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>78332.625</td>\n",
       "      <td>0.088435</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>8526</td>\n",
       "      <td>0.206346</td>\n",
       "      <td>0.042579</td>\n",
       "      <td>0.050021</td>\n",
       "      <td>8.067022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033991</td>\n",
       "      <td>6.509871e-06</td>\n",
       "      <td>0.134328</td>\n",
       "      <td>0.054790</td>\n",
       "      <td>1.368962</td>\n",
       "      <td>0.759666</td>\n",
       "      <td>4.200000e+00</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.680296</td>\n",
       "      <td>-0.152630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10012</td>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>39836.000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>20897</td>\n",
       "      <td>2.470629</td>\n",
       "      <td>6.104008</td>\n",
       "      <td>-1.170178</td>\n",
       "      <td>11.360419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138777</td>\n",
       "      <td>9.482216e-08</td>\n",
       "      <td>0.129412</td>\n",
       "      <td>0.075671</td>\n",
       "      <td>0.465917</td>\n",
       "      <td>7.007556</td>\n",
       "      <td>8.756593e-09</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.061049</td>\n",
       "      <td>-0.397365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10016</td>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>379569.500</td>\n",
       "      <td>0.030726</td>\n",
       "      <td>23.0625</td>\n",
       "      <td>16964</td>\n",
       "      <td>0.449866</td>\n",
       "      <td>0.202379</td>\n",
       "      <td>0.391222</td>\n",
       "      <td>12.024414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054578</td>\n",
       "      <td>5.643552e-08</td>\n",
       "      <td>0.070769</td>\n",
       "      <td>0.040708</td>\n",
       "      <td>1.242227</td>\n",
       "      <td>8.102766</td>\n",
       "      <td>1.833562e-08</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>-0.227582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10019</td>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>28945.000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>3.7500</td>\n",
       "      <td>8270</td>\n",
       "      <td>2.249729</td>\n",
       "      <td>5.061279</td>\n",
       "      <td>0.203106</td>\n",
       "      <td>9.294773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131620</td>\n",
       "      <td>3.206363e-07</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.120324</td>\n",
       "      <td>0.983488</td>\n",
       "      <td>16.163956</td>\n",
       "      <td>7.497863e-09</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.552262</td>\n",
       "      <td>0.036872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   permno       DATE       mvel1       RET      prc  SHROUT      beta  \\\n",
       "0   10001 2001-01-31   24355.500  0.012821   9.8750    2498  0.037079   \n",
       "1   10002 2001-01-31   78332.625  0.088435  10.0000    8526  0.206346   \n",
       "2   10012 2001-01-31   39836.000  0.500000   3.0000   20897  2.470629   \n",
       "3   10016 2001-01-31  379569.500  0.030726  23.0625   16964  0.449866   \n",
       "4   10019 2001-01-31   28945.000  0.071429   3.7500    8270  2.249729   \n",
       "\n",
       "     betasq     chmom     dolvol  ...  baspread           ill    maxret  \\\n",
       "0  0.001375  0.281788   8.395576  ...  0.020711  1.098587e-06  0.027778   \n",
       "1  0.042579  0.050021   8.067022  ...  0.033991  6.509871e-06  0.134328   \n",
       "2  6.104008 -1.170178  11.360419  ...  0.138777  9.482216e-08  0.129412   \n",
       "3  0.202379  0.391222  12.024414  ...  0.054578  5.643552e-08  0.070769   \n",
       "4  5.061279  0.203106   9.294773  ...  0.131620  3.206363e-07  0.435897   \n",
       "\n",
       "     retvol  std_dolvol   std_turn     zerotrade  sic2        bm     bm_ia  \n",
       "0  0.017710    0.972710   0.426715  4.200000e+00  49.0  0.868139 -0.104925  \n",
       "1  0.054790    1.368962   0.759666  4.200000e+00  60.0  0.680296 -0.152630  \n",
       "2  0.075671    0.465917   7.007556  8.756593e-09  36.0  0.061049 -0.397365  \n",
       "3  0.040708    1.242227   8.102766  1.833562e-08  38.0  0.287808 -0.227582  \n",
       "4  0.120324    0.983488  16.163956  7.497863e-09  38.0  0.552262  0.036872  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# start date and end date of the sample\n",
    "#stdt, nddt = 19570101, 20161231\n",
    "stdt, nddt = 20010101, 20201231\n",
    "\n",
    "# load firm characteristics data\n",
    "data_ch = pd.read_csv('GKX_20201231.csv')\n",
    "data_ch = data_ch[(data_ch['DATE']>=stdt)&(data_ch['DATE']<=nddt)].reset_index(drop=True)\n",
    "data_ch['DATE'] = pd.to_datetime(data_ch['DATE'],format='%Y%m%d')+pd.offsets.MonthEnd(0)\n",
    "characteristics = list(set(data_ch.columns).difference({'permno','DATE','SHROUT','mve0','sic2','RET','prc'}))\n",
    "\n",
    "data_ch.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba05873f",
   "metadata": {},
   "source": [
    "### Pick out Top 1000 and Bottom 1000 Firms\n",
    "\n",
    "Next, let's pick out the top 1000 and bottom 1000 firms with respect to market capitalization to see the differnce of predictability between big firms and small firms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6774130",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ch_top = data_ch.sort_values('mvel1',ascending=False).groupby('DATE').head(1000).reset_index(drop=True)\n",
    "data_ch_bot = data_ch.sort_values('mvel1',ascending=False).groupby('DATE').tail(1000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb154c2",
   "metadata": {},
   "source": [
    "### Missing Characteristics\n",
    "According to the paper, the __missing data__ are replaced by the _cross-sectional median_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa34628a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "permno            0\n",
       "DATE              0\n",
       "mvel1           276\n",
       "RET               0\n",
       "prc            7830\n",
       "              ...  \n",
       "std_turn        410\n",
       "zerotrade       342\n",
       "sic2          22532\n",
       "bm           375825\n",
       "bm_ia        375825\n",
       "Length: 101, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing data before filling\n",
    "data_ch.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8350b08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.15 s, sys: 3.33 ms, total: 6.16 s\n",
      "Wall time: 6.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fill na with cross-sectional median\n",
    "for ch in characteristics:\n",
    "     data_ch[ch] = data_ch.groupby('DATE')[ch].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2306eb7",
   "metadata": {},
   "source": [
    "Since there are some characeristics that are all missing at some time point, we still encounter missing data after the filling process. Then, let's try to fill the remaining na with time-series median. Unfortunately, after filling na with time-series median, na still exists. Since there is no further instruction of how to deal with remaining na in the data, after consulting some replication code online, I __fill the remaining na with 0__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c11dba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['prc', 'mve0', 'sic2'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ch in characteristics:\n",
    "     data_ch[ch] = data_ch[ch].fillna(0)\n",
    "    \n",
    "data_ch.columns[data_ch.isnull().sum()!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4916e289",
   "metadata": {},
   "source": [
    "Now, we do not have missing characteristics in our dataset. Then, do the same process to top and bottom 1000 firms data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce3803f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na(data_ch, characteristics):\n",
    "    for ch in characteristics:\n",
    "         data_ch[ch] = data_ch.groupby('DATE')[ch].transform(lambda x: x.fillna(x.median()))\n",
    "    for ch in characteristics:\n",
    "         data_ch[ch] = data_ch[ch].fillna(0)\n",
    "    return data_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cad5e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ch_top = fill_na(data_ch_top, characteristics)\n",
    "data_ch_bot = fill_na(data_ch_bot, characteristics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08a6e144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>DATE</th>\n",
       "      <th>mvel1</th>\n",
       "      <th>RET</th>\n",
       "      <th>prc</th>\n",
       "      <th>SHROUT</th>\n",
       "      <th>beta</th>\n",
       "      <th>betasq</th>\n",
       "      <th>chmom</th>\n",
       "      <th>dolvol</th>\n",
       "      <th>...</th>\n",
       "      <th>baspread</th>\n",
       "      <th>ill</th>\n",
       "      <th>maxret</th>\n",
       "      <th>retvol</th>\n",
       "      <th>std_dolvol</th>\n",
       "      <th>std_turn</th>\n",
       "      <th>zerotrade</th>\n",
       "      <th>sic2</th>\n",
       "      <th>bm</th>\n",
       "      <th>bm_ia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93436</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>1.735365e+08</td>\n",
       "      <td>0.243252</td>\n",
       "      <td>705.669983</td>\n",
       "      <td>947901</td>\n",
       "      <td>1.349505</td>\n",
       "      <td>1.821163</td>\n",
       "      <td>0.868039</td>\n",
       "      <td>19.036016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047931</td>\n",
       "      <td>1.820359e-12</td>\n",
       "      <td>0.101968</td>\n",
       "      <td>0.039271</td>\n",
       "      <td>0.531632</td>\n",
       "      <td>18.110463</td>\n",
       "      <td>2.654462e-09</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55976</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>1.735365e+08</td>\n",
       "      <td>-0.053014</td>\n",
       "      <td>144.149994</td>\n",
       "      <td>2829286</td>\n",
       "      <td>0.180955</td>\n",
       "      <td>0.032745</td>\n",
       "      <td>0.180443</td>\n",
       "      <td>19.002931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017848</td>\n",
       "      <td>1.079382e-11</td>\n",
       "      <td>0.020323</td>\n",
       "      <td>0.011571</td>\n",
       "      <td>0.355476</td>\n",
       "      <td>0.954404</td>\n",
       "      <td>4.492710e-08</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59408</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>1.735365e+08</td>\n",
       "      <td>0.082741</td>\n",
       "      <td>30.309999</td>\n",
       "      <td>8650790</td>\n",
       "      <td>1.259201</td>\n",
       "      <td>1.585588</td>\n",
       "      <td>0.451376</td>\n",
       "      <td>19.036016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026698</td>\n",
       "      <td>1.163103e-11</td>\n",
       "      <td>0.141917</td>\n",
       "      <td>0.039133</td>\n",
       "      <td>0.371290</td>\n",
       "      <td>3.542247</td>\n",
       "      <td>1.473082e-08</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62092</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>1.735365e+08</td>\n",
       "      <td>0.002194</td>\n",
       "      <td>465.779999</td>\n",
       "      <td>396335</td>\n",
       "      <td>0.610849</td>\n",
       "      <td>0.373137</td>\n",
       "      <td>0.219307</td>\n",
       "      <td>18.643992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032433</td>\n",
       "      <td>1.943569e-11</td>\n",
       "      <td>0.039613</td>\n",
       "      <td>0.029300</td>\n",
       "      <td>0.373314</td>\n",
       "      <td>2.134823</td>\n",
       "      <td>2.033140e-08</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65875</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>1.735365e+08</td>\n",
       "      <td>-0.027479</td>\n",
       "      <td>58.750000</td>\n",
       "      <td>4138094</td>\n",
       "      <td>0.364416</td>\n",
       "      <td>0.132799</td>\n",
       "      <td>0.103106</td>\n",
       "      <td>19.036016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012642</td>\n",
       "      <td>7.004889e-12</td>\n",
       "      <td>0.022382</td>\n",
       "      <td>0.008920</td>\n",
       "      <td>0.335083</td>\n",
       "      <td>1.166184</td>\n",
       "      <td>3.099543e-08</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239995</th>\n",
       "      <td>79037</td>\n",
       "      <td>2002-10-31</td>\n",
       "      <td>1.222173e+06</td>\n",
       "      <td>-0.007726</td>\n",
       "      <td>32.049999</td>\n",
       "      <td>37768</td>\n",
       "      <td>0.581556</td>\n",
       "      <td>0.338207</td>\n",
       "      <td>-0.534422</td>\n",
       "      <td>14.693386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037615</td>\n",
       "      <td>2.115648e-09</td>\n",
       "      <td>0.079597</td>\n",
       "      <td>0.031758</td>\n",
       "      <td>0.542623</td>\n",
       "      <td>6.748285</td>\n",
       "      <td>1.181658e-08</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.310390</td>\n",
       "      <td>-0.729187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239996</th>\n",
       "      <td>64515</td>\n",
       "      <td>2002-10-31</td>\n",
       "      <td>1.219650e+06</td>\n",
       "      <td>0.035871</td>\n",
       "      <td>25.990000</td>\n",
       "      <td>48735</td>\n",
       "      <td>0.595881</td>\n",
       "      <td>0.355074</td>\n",
       "      <td>-0.278501</td>\n",
       "      <td>13.877183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031244</td>\n",
       "      <td>5.555383e-09</td>\n",
       "      <td>0.029286</td>\n",
       "      <td>0.021217</td>\n",
       "      <td>0.479610</td>\n",
       "      <td>2.382454</td>\n",
       "      <td>3.255539e-08</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.679427</td>\n",
       "      <td>-0.094849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239997</th>\n",
       "      <td>18649</td>\n",
       "      <td>2002-10-31</td>\n",
       "      <td>1.215267e+06</td>\n",
       "      <td>-0.054911</td>\n",
       "      <td>21.170000</td>\n",
       "      <td>54253</td>\n",
       "      <td>0.539772</td>\n",
       "      <td>0.291354</td>\n",
       "      <td>-0.495734</td>\n",
       "      <td>13.622397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023666</td>\n",
       "      <td>2.646572e-09</td>\n",
       "      <td>0.020842</td>\n",
       "      <td>0.013352</td>\n",
       "      <td>0.439718</td>\n",
       "      <td>1.460588</td>\n",
       "      <td>2.948899e-08</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.405229</td>\n",
       "      <td>-0.221642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239998</th>\n",
       "      <td>89260</td>\n",
       "      <td>2002-10-31</td>\n",
       "      <td>1.214574e+06</td>\n",
       "      <td>-0.030085</td>\n",
       "      <td>22.889999</td>\n",
       "      <td>51465</td>\n",
       "      <td>0.505972</td>\n",
       "      <td>0.256008</td>\n",
       "      <td>-0.398208</td>\n",
       "      <td>13.336064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030121</td>\n",
       "      <td>5.554476e-09</td>\n",
       "      <td>0.038286</td>\n",
       "      <td>0.025303</td>\n",
       "      <td>0.652586</td>\n",
       "      <td>3.016302</td>\n",
       "      <td>2.316407e-08</td>\n",
       "      <td>37.0</td>\n",
       "      <td>-0.155999</td>\n",
       "      <td>-0.993152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239999</th>\n",
       "      <td>83616</td>\n",
       "      <td>2002-10-31</td>\n",
       "      <td>1.214283e+06</td>\n",
       "      <td>0.254808</td>\n",
       "      <td>26.100000</td>\n",
       "      <td>58379</td>\n",
       "      <td>2.626624</td>\n",
       "      <td>6.899153</td>\n",
       "      <td>-1.087871</td>\n",
       "      <td>15.209024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062978</td>\n",
       "      <td>1.505120e-09</td>\n",
       "      <td>0.073766</td>\n",
       "      <td>0.036783</td>\n",
       "      <td>0.370787</td>\n",
       "      <td>8.072226</td>\n",
       "      <td>5.631389e-09</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.058915</td>\n",
       "      <td>-0.495477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240000 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        permno       DATE         mvel1       RET         prc   SHROUT  \\\n",
       "0        93436 2020-12-31  1.735365e+08  0.243252  705.669983   947901   \n",
       "1        55976 2020-12-31  1.735365e+08 -0.053014  144.149994  2829286   \n",
       "2        59408 2020-12-31  1.735365e+08  0.082741   30.309999  8650790   \n",
       "3        62092 2020-12-31  1.735365e+08  0.002194  465.779999   396335   \n",
       "4        65875 2020-12-31  1.735365e+08 -0.027479   58.750000  4138094   \n",
       "...        ...        ...           ...       ...         ...      ...   \n",
       "239995   79037 2002-10-31  1.222173e+06 -0.007726   32.049999    37768   \n",
       "239996   64515 2002-10-31  1.219650e+06  0.035871   25.990000    48735   \n",
       "239997   18649 2002-10-31  1.215267e+06 -0.054911   21.170000    54253   \n",
       "239998   89260 2002-10-31  1.214574e+06 -0.030085   22.889999    51465   \n",
       "239999   83616 2002-10-31  1.214283e+06  0.254808   26.100000    58379   \n",
       "\n",
       "            beta    betasq     chmom     dolvol  ...  baspread           ill  \\\n",
       "0       1.349505  1.821163  0.868039  19.036016  ...  0.047931  1.820359e-12   \n",
       "1       0.180955  0.032745  0.180443  19.002931  ...  0.017848  1.079382e-11   \n",
       "2       1.259201  1.585588  0.451376  19.036016  ...  0.026698  1.163103e-11   \n",
       "3       0.610849  0.373137  0.219307  18.643992  ...  0.032433  1.943569e-11   \n",
       "4       0.364416  0.132799  0.103106  19.036016  ...  0.012642  7.004889e-12   \n",
       "...          ...       ...       ...        ...  ...       ...           ...   \n",
       "239995  0.581556  0.338207 -0.534422  14.693386  ...  0.037615  2.115648e-09   \n",
       "239996  0.595881  0.355074 -0.278501  13.877183  ...  0.031244  5.555383e-09   \n",
       "239997  0.539772  0.291354 -0.495734  13.622397  ...  0.023666  2.646572e-09   \n",
       "239998  0.505972  0.256008 -0.398208  13.336064  ...  0.030121  5.554476e-09   \n",
       "239999  2.626624  6.899153 -1.087871  15.209024  ...  0.062978  1.505120e-09   \n",
       "\n",
       "          maxret    retvol  std_dolvol   std_turn     zerotrade  sic2  \\\n",
       "0       0.101968  0.039271    0.531632  18.110463  2.654462e-09  37.0   \n",
       "1       0.020323  0.011571    0.355476   0.954404  4.492710e-08  53.0   \n",
       "2       0.141917  0.039133    0.371290   3.542247  1.473082e-08  60.0   \n",
       "3       0.039613  0.029300    0.373314   2.134823  2.033140e-08  38.0   \n",
       "4       0.022382  0.008920    0.335083   1.166184  3.099543e-08  48.0   \n",
       "...          ...       ...         ...        ...           ...   ...   \n",
       "239995  0.079597  0.031758    0.542623   6.748285  1.181658e-08  25.0   \n",
       "239996  0.029286  0.021217    0.479610   2.382454  3.255539e-08  60.0   \n",
       "239997  0.020842  0.013352    0.439718   1.460588  2.948899e-08  73.0   \n",
       "239998  0.038286  0.025303    0.652586   3.016302  2.316407e-08  37.0   \n",
       "239999  0.073766  0.036783    0.370787   8.072226  5.631389e-09  38.0   \n",
       "\n",
       "              bm     bm_ia  \n",
       "0       0.000000  0.000000  \n",
       "1       0.000000  0.000000  \n",
       "2       0.000000  0.000000  \n",
       "3       0.000000  0.000000  \n",
       "4       0.000000  0.000000  \n",
       "...          ...       ...  \n",
       "239995  0.310390 -0.729187  \n",
       "239996  0.679427 -0.094849  \n",
       "239997  0.405229 -0.221642  \n",
       "239998 -0.155999 -0.993152  \n",
       "239999  0.058915 -0.495477  \n",
       "\n",
       "[240000 rows x 101 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ch_top"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcef889",
   "metadata": {},
   "source": [
    "### Transform SIC Code into Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b86c7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dummies for SIC code\n",
    "def get_sic_dummies(data_ch):\n",
    "    sic_dummies = pd.get_dummies(data_ch['sic2'].fillna(999).astype(int),prefix='sic').drop('sic_999',axis=1)\n",
    "    data_ch_d = pd.concat([data_ch,sic_dummies],axis=1)\n",
    "    data_ch_d.drop(['prc','SHROUT','mve0','sic2'],inplace=True,axis=1)\n",
    "    return data_ch_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb13ea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ch_d = get_sic_dummies(data_ch)\n",
    "data_ch_top_d = get_sic_dummies(data_ch_top)\n",
    "data_ch_bot_d = get_sic_dummies(data_ch_bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c4bc5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['permno', 'DATE', 'mvel1', 'RET', 'beta', 'betasq', 'chmom', 'dolvol',\n",
       "       'idiovol', 'indmom',\n",
       "       ...\n",
       "       'sic_78', 'sic_79', 'sic_80', 'sic_81', 'sic_82', 'sic_83', 'sic_86',\n",
       "       'sic_87', 'sic_89', 'sic_99'],\n",
       "      dtype='object', length=170)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ch_d.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626cd875",
   "metadata": {},
   "source": [
    "### Macroeconomic Predictors Data\n",
    "\n",
    "The eight macroeconomic predictors follows the definitions by Welch and Goyal (2008, RFS). The data are available on Prof Goyal's [website](https://sites.google.com/view/agoyal145)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d624dc9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yyyymm</th>\n",
       "      <th>Index</th>\n",
       "      <th>D12</th>\n",
       "      <th>E12</th>\n",
       "      <th>b/m</th>\n",
       "      <th>tbl</th>\n",
       "      <th>AAA</th>\n",
       "      <th>BAA</th>\n",
       "      <th>lty</th>\n",
       "      <th>ntis</th>\n",
       "      <th>Rfree</th>\n",
       "      <th>infl</th>\n",
       "      <th>ltr</th>\n",
       "      <th>corpr</th>\n",
       "      <th>svar</th>\n",
       "      <th>csp</th>\n",
       "      <th>CRSP_SPvw</th>\n",
       "      <th>CRSP_SPvwx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200101</td>\n",
       "      <td>1,366.01</td>\n",
       "      <td>16.1717</td>\n",
       "      <td>48.4800</td>\n",
       "      <td>0.150450</td>\n",
       "      <td>0.0515</td>\n",
       "      <td>0.0715</td>\n",
       "      <td>0.0793</td>\n",
       "      <td>0.0562</td>\n",
       "      <td>-0.003193</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.006322</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>0.004941</td>\n",
       "      <td>-0.001565</td>\n",
       "      <td>0.032372</td>\n",
       "      <td>0.031555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200102</td>\n",
       "      <td>1,239.94</td>\n",
       "      <td>16.0723</td>\n",
       "      <td>46.9600</td>\n",
       "      <td>0.156070</td>\n",
       "      <td>0.0488</td>\n",
       "      <td>0.0710</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>0.0549</td>\n",
       "      <td>-0.006856</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.003998</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>-0.002273</td>\n",
       "      <td>-0.090952</td>\n",
       "      <td>-0.092131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200103</td>\n",
       "      <td>1,160.33</td>\n",
       "      <td>15.9730</td>\n",
       "      <td>45.4400</td>\n",
       "      <td>0.133114</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.0698</td>\n",
       "      <td>0.0784</td>\n",
       "      <td>0.0559</td>\n",
       "      <td>-0.005213</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.002275</td>\n",
       "      <td>-0.0074</td>\n",
       "      <td>-0.0029</td>\n",
       "      <td>0.007140</td>\n",
       "      <td>-0.001864</td>\n",
       "      <td>-0.063706</td>\n",
       "      <td>-0.064587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200104</td>\n",
       "      <td>1,249.46</td>\n",
       "      <td>15.8773</td>\n",
       "      <td>42.5567</td>\n",
       "      <td>0.122497</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>0.0720</td>\n",
       "      <td>0.0807</td>\n",
       "      <td>0.0593</td>\n",
       "      <td>-0.002543</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.003973</td>\n",
       "      <td>-0.0313</td>\n",
       "      <td>-0.0128</td>\n",
       "      <td>0.007426</td>\n",
       "      <td>-0.001025</td>\n",
       "      <td>0.077824</td>\n",
       "      <td>0.076981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200105</td>\n",
       "      <td>1,255.82</td>\n",
       "      <td>15.7817</td>\n",
       "      <td>39.6733</td>\n",
       "      <td>0.120510</td>\n",
       "      <td>0.0362</td>\n",
       "      <td>0.0729</td>\n",
       "      <td>0.0807</td>\n",
       "      <td>0.0594</td>\n",
       "      <td>-0.000248</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.004522</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.002536</td>\n",
       "      <td>-0.001184</td>\n",
       "      <td>0.006859</td>\n",
       "      <td>0.005330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>202008</td>\n",
       "      <td>3,500.31</td>\n",
       "      <td>59.1286</td>\n",
       "      <td>98.5567</td>\n",
       "      <td>0.232933</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.0327</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>-0.008504</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.003153</td>\n",
       "      <td>-0.0349</td>\n",
       "      <td>-0.0488</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.072068</td>\n",
       "      <td>0.070308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>202009</td>\n",
       "      <td>3,363.00</td>\n",
       "      <td>58.8512</td>\n",
       "      <td>98.2200</td>\n",
       "      <td>0.238369</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0231</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>-0.005698</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.004907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.038151</td>\n",
       "      <td>-0.039366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>202010</td>\n",
       "      <td>3,269.96</td>\n",
       "      <td>58.6604</td>\n",
       "      <td>96.8567</td>\n",
       "      <td>0.249883</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>-0.001895</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>-0.0238</td>\n",
       "      <td>-0.0190</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.026409</td>\n",
       "      <td>-0.027507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>202011</td>\n",
       "      <td>3,621.63</td>\n",
       "      <td>58.4696</td>\n",
       "      <td>95.4933</td>\n",
       "      <td>0.223435</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0330</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>-0.005262</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.000611</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0509</td>\n",
       "      <td>0.002492</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.109404</td>\n",
       "      <td>0.107624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>202012</td>\n",
       "      <td>3,756.07</td>\n",
       "      <td>58.2788</td>\n",
       "      <td>94.1300</td>\n",
       "      <td>0.216370</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0226</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>-0.000098</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>-0.0115</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.041572</td>\n",
       "      <td>0.040137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     yyyymm     Index      D12      E12       b/m     tbl     AAA     BAA  \\\n",
       "0    200101  1,366.01  16.1717  48.4800  0.150450  0.0515  0.0715  0.0793   \n",
       "1    200102  1,239.94  16.0723  46.9600  0.156070  0.0488  0.0710  0.0787   \n",
       "2    200103  1,160.33  15.9730  45.4400  0.133114  0.0442  0.0698  0.0784   \n",
       "3    200104  1,249.46  15.8773  42.5567  0.122497  0.0387  0.0720  0.0807   \n",
       "4    200105  1,255.82  15.7817  39.6733  0.120510  0.0362  0.0729  0.0807   \n",
       "..      ...       ...      ...      ...       ...     ...     ...     ...   \n",
       "235  202008  3,500.31  59.1286  98.5567  0.232933  0.0010  0.0225  0.0327   \n",
       "236  202009  3,363.00  58.8512  98.2200  0.238369  0.0011  0.0231  0.0336   \n",
       "237  202010  3,269.96  58.6604  96.8567  0.249883  0.0010  0.0235  0.0344   \n",
       "238  202011  3,621.63  58.4696  95.4933  0.223435  0.0009  0.0230  0.0330   \n",
       "239  202012  3,756.07  58.2788  94.1300  0.216370  0.0009  0.0226  0.0316   \n",
       "\n",
       "        lty      ntis   Rfree      infl     ltr   corpr      svar       csp  \\\n",
       "0    0.0562 -0.003193  0.0054  0.006322  0.0005  0.0359  0.004941 -0.001565   \n",
       "1    0.0549 -0.006856  0.0038  0.003998  0.0191  0.0127  0.002528 -0.002273   \n",
       "2    0.0559 -0.005213  0.0042  0.002275 -0.0074 -0.0029  0.007140 -0.001864   \n",
       "3    0.0593 -0.002543  0.0039  0.003973 -0.0313 -0.0128  0.007426 -0.001025   \n",
       "4    0.0594 -0.000248  0.0032  0.004522  0.0037  0.0132  0.002536 -0.001184   \n",
       "..      ...       ...     ...       ...     ...     ...       ...       ...   \n",
       "235  0.0065 -0.008504  0.0001  0.003153 -0.0349 -0.0488  0.000743       NaN   \n",
       "236  0.0068 -0.005698  0.0001  0.001393  0.0080  0.0041  0.004907       NaN   \n",
       "237  0.0079 -0.001895  0.0001  0.000415 -0.0238 -0.0190  0.003661       NaN   \n",
       "238  0.0087 -0.005262  0.0001 -0.000611  0.0093  0.0509  0.002492       NaN   \n",
       "239  0.0093 -0.000098  0.0001  0.000941 -0.0115  0.0000  0.000678       NaN   \n",
       "\n",
       "     CRSP_SPvw  CRSP_SPvwx  \n",
       "0     0.032372    0.031555  \n",
       "1    -0.090952   -0.092131  \n",
       "2    -0.063706   -0.064587  \n",
       "3     0.077824    0.076981  \n",
       "4     0.006859    0.005330  \n",
       "..         ...         ...  \n",
       "235   0.072068    0.070308  \n",
       "236  -0.038151   -0.039366  \n",
       "237  -0.026409   -0.027507  \n",
       "238   0.109404    0.107624  \n",
       "239   0.041572    0.040137  \n",
       "\n",
       "[240 rows x 18 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ma = pd.read_csv('PredictorData2023.csv')\n",
    "data_ma = data_ma[(data_ma['yyyymm']>=stdt//100)&(data_ma['yyyymm']<=nddt//100)].reset_index(drop=True)\n",
    "data_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "748354d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['yyyymm', 'Index', 'D12', 'E12', 'b/m', 'tbl', 'AAA', 'BAA', 'lty',\n",
       "       'ntis', 'Rfree', 'infl', 'ltr', 'corpr', 'svar', 'csp', 'CRSP_SPvw',\n",
       "       'CRSP_SPvwx'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ma.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a44ddfa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yyyymm</th>\n",
       "      <th>dp_sp</th>\n",
       "      <th>ep_sp</th>\n",
       "      <th>bm_sp</th>\n",
       "      <th>ntis</th>\n",
       "      <th>tbl</th>\n",
       "      <th>tms</th>\n",
       "      <th>dfy</th>\n",
       "      <th>svar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>0.011839</td>\n",
       "      <td>0.035490</td>\n",
       "      <td>0.150450</td>\n",
       "      <td>-0.003193</td>\n",
       "      <td>0.0515</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.004941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-02-28</td>\n",
       "      <td>0.012962</td>\n",
       "      <td>0.037873</td>\n",
       "      <td>0.156070</td>\n",
       "      <td>-0.006856</td>\n",
       "      <td>0.0488</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.002528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-03-31</td>\n",
       "      <td>0.013766</td>\n",
       "      <td>0.039161</td>\n",
       "      <td>0.133114</td>\n",
       "      <td>-0.005213</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.007140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>0.012707</td>\n",
       "      <td>0.034060</td>\n",
       "      <td>0.122497</td>\n",
       "      <td>-0.002543</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>0.0206</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.007426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001-05-31</td>\n",
       "      <td>0.012567</td>\n",
       "      <td>0.031592</td>\n",
       "      <td>0.120510</td>\n",
       "      <td>-0.000248</td>\n",
       "      <td>0.0362</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.002536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      yyyymm     dp_sp     ep_sp     bm_sp      ntis     tbl     tms     dfy  \\\n",
       "0 2001-01-31  0.011839  0.035490  0.150450 -0.003193  0.0515  0.0047  0.0078   \n",
       "1 2001-02-28  0.012962  0.037873  0.156070 -0.006856  0.0488  0.0061  0.0077   \n",
       "2 2001-03-31  0.013766  0.039161  0.133114 -0.005213  0.0442  0.0117  0.0086   \n",
       "3 2001-04-30  0.012707  0.034060  0.122497 -0.002543  0.0387  0.0206  0.0087   \n",
       "4 2001-05-31  0.012567  0.031592  0.120510 -0.000248  0.0362  0.0232  0.0078   \n",
       "\n",
       "       svar  \n",
       "0  0.004941  \n",
       "1  0.002528  \n",
       "2  0.007140  \n",
       "3  0.007426  \n",
       "4  0.002536  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load macroeconomic predictors data\n",
    "data_ma = pd.read_csv('PredictorData2023.csv')\n",
    "data_ma = data_ma[(data_ma['yyyymm']>=stdt//100)&(data_ma['yyyymm']<=nddt//100)].reset_index(drop=True)\n",
    "\n",
    "# construct predictor\n",
    "ma_predictors = ['dp_sp','ep_sp','bm_sp','ntis','tbl','tms','dfy','svar']\n",
    "data_ma['Index'] = data_ma['Index'].str.replace(',','').astype('float64')\n",
    "data_ma['dp_sp'] = data_ma['D12']/data_ma['Index']\n",
    "data_ma['ep_sp'] = data_ma['E12']/data_ma['Index']\n",
    "data_ma.rename({'b/m':'bm_sp'},axis=1,inplace=True)\n",
    "data_ma['tms'] = data_ma['lty']-data_ma['tbl']\n",
    "data_ma['dfy'] = data_ma['BAA']-data_ma['AAA']\n",
    "data_ma = data_ma[['yyyymm']+ma_predictors]\n",
    "data_ma['yyyymm'] = pd.to_datetime(data_ma['yyyymm'],format='%Y%m')+pd.offsets.MonthEnd(0)\n",
    "data_ma.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8233ddf4",
   "metadata": {},
   "source": [
    "### Construct the Dataset including all the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf1bfd0",
   "metadata": {},
   "source": [
    "Besides adding the interaction terms, this function also transform the data into (-1, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbaaa1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def interactions(data_ch, data_ma, characteristics, ma_predictors, minmax=True):\n",
    "    # construct interactions between firm characteristics and macroeconomic predictors\n",
    "    data = data_ch.copy()\n",
    "    data_ma_long = pd.merge(data[['DATE']],data_ma,left_on='DATE',right_on='yyyymm',how='left')\n",
    "    data = data.reset_index(drop=True)\n",
    "    data_ma_long = data_ma_long.reset_index(drop=True)\n",
    "    for fc in characteristics:\n",
    "        for mp in ma_predictors:\n",
    "            data[fc+'*'+mp] = data[fc]*data_ma_long[mp]\n",
    "\n",
    "    features = list(set(data.columns).difference({'permno','DATE','RET'})) # a list storing all 920 features used\n",
    "    if minmax:\n",
    "        X = MinMaxScaler((-1,1)).fit_transform(data[features])\n",
    "        X = pd.DataFrame(X, columns=features)\n",
    "    else:\n",
    "        X = data[features]\n",
    "    y = data['RET']\n",
    "    print(f\"The shape of the data is: {data.shape}\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c2a5c",
   "metadata": {},
   "source": [
    "### Split the Sample into Training Set, Validation Set and Testing Set\n",
    "\n",
    "According to the paper, the authors use first 18 years (1957-1974) for training, last 30 years (1987-2016) for out-of-sample testing, and the 12 years in the middle (1975-1986) for tuning hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f2bf0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stdt_vld = np.datetime64('1975-01-31')\n",
    "#stdt_tst = np.datetime64('1987-01-31')\n",
    "stdt_vld = np.datetime64('2009-01-31')\n",
    "stdt_tst = np.datetime64('2015-01-31')\n",
    "\n",
    "def trn_vld_tst(data):\n",
    "\n",
    "    # training setstdt_vld = np.datetime64('2001-01-31')\n",
    "    X_trn, y_trn = interactions(data[data['DATE']<stdt_vld],data_ma[data_ma['yyyymm']<stdt_vld],characteristics,ma_predictors)\n",
    "\n",
    "    # validation set\n",
    "    X_vld, y_vld = interactions(data[(data['DATE']<stdt_tst)&(data['DATE']>=stdt_vld)],data_ma[(data_ma['yyyymm']<stdt_tst)&(data_ma['yyyymm']>=stdt_vld)],characteristics,ma_predictors)\n",
    "\n",
    "    # testing set\n",
    "    X_tst, y_tst = interactions(data[data['DATE']>=stdt_tst],data_ma[data_ma['yyyymm']>=stdt_tst],characteristics,ma_predictors)\n",
    "    return X_trn, X_vld, X_tst, y_trn, y_vld, y_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dd2b54f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the data is: (96000, 915)\n",
      "The shape of the data is: (72000, 915)\n",
      "The shape of the data is: (72000, 915)\n",
      "CPU times: user 5.29 s, sys: 3.28 s, total: 8.58 s\n",
      "Wall time: 8.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_trn, X_vld, X_tst, y_trn, y_vld, y_tst = trn_vld_tst(data_ch_top_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbf861c",
   "metadata": {},
   "source": [
    "The differnce in the number of features results from the number of dummies from SIC code in the sample period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e23ba926",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maxret</th>\n",
       "      <th>sgr*tbl</th>\n",
       "      <th>invest*svar</th>\n",
       "      <th>cashpr*tms</th>\n",
       "      <th>pchcurrat*bm_sp</th>\n",
       "      <th>pchdepr*svar</th>\n",
       "      <th>maxret*ntis</th>\n",
       "      <th>absacc*ntis</th>\n",
       "      <th>cashdebt*dp_sp</th>\n",
       "      <th>roic*tms</th>\n",
       "      <th>...</th>\n",
       "      <th>maxret*ep_sp</th>\n",
       "      <th>maxret*svar</th>\n",
       "      <th>salecash*tbl</th>\n",
       "      <th>pchsale_pchinvt*svar</th>\n",
       "      <th>sic_60</th>\n",
       "      <th>chinv*svar</th>\n",
       "      <th>idiovol*dp_sp</th>\n",
       "      <th>pchsale_pchxsga*tms</th>\n",
       "      <th>herf*dp_sp</th>\n",
       "      <th>tang*dp_sp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.952404</td>\n",
       "      <td>-0.792172</td>\n",
       "      <td>-0.680108</td>\n",
       "      <td>-0.408892</td>\n",
       "      <td>-0.720001</td>\n",
       "      <td>-0.800710</td>\n",
       "      <td>0.484978</td>\n",
       "      <td>0.023247</td>\n",
       "      <td>-0.056024</td>\n",
       "      <td>0.558184</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.903560</td>\n",
       "      <td>-0.998373</td>\n",
       "      <td>-0.994198</td>\n",
       "      <td>0.487698</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.063661</td>\n",
       "      <td>-0.762035</td>\n",
       "      <td>-0.322091</td>\n",
       "      <td>-0.978324</td>\n",
       "      <td>-0.616306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.948345</td>\n",
       "      <td>-0.862089</td>\n",
       "      <td>-0.681235</td>\n",
       "      <td>-0.412348</td>\n",
       "      <td>-0.753810</td>\n",
       "      <td>-0.800555</td>\n",
       "      <td>0.484345</td>\n",
       "      <td>0.026845</td>\n",
       "      <td>-0.068120</td>\n",
       "      <td>0.552080</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.895688</td>\n",
       "      <td>-0.998244</td>\n",
       "      <td>-0.994507</td>\n",
       "      <td>0.492519</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.066832</td>\n",
       "      <td>-0.866056</td>\n",
       "      <td>-0.324468</td>\n",
       "      <td>-0.783369</td>\n",
       "      <td>-0.325258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.968391</td>\n",
       "      <td>-0.805653</td>\n",
       "      <td>-0.677884</td>\n",
       "      <td>-0.399741</td>\n",
       "      <td>-0.772443</td>\n",
       "      <td>-0.801024</td>\n",
       "      <td>0.487473</td>\n",
       "      <td>0.032162</td>\n",
       "      <td>-0.057052</td>\n",
       "      <td>0.558757</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.934565</td>\n",
       "      <td>-0.998882</td>\n",
       "      <td>-0.856003</td>\n",
       "      <td>0.488916</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.060345</td>\n",
       "      <td>-0.830636</td>\n",
       "      <td>-0.323207</td>\n",
       "      <td>-0.384286</td>\n",
       "      <td>-0.436603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.923763</td>\n",
       "      <td>-0.773741</td>\n",
       "      <td>-0.672744</td>\n",
       "      <td>-0.412169</td>\n",
       "      <td>-0.761726</td>\n",
       "      <td>-0.801338</td>\n",
       "      <td>0.480509</td>\n",
       "      <td>-0.003747</td>\n",
       "      <td>-0.063439</td>\n",
       "      <td>0.552081</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.848018</td>\n",
       "      <td>-0.997461</td>\n",
       "      <td>-0.992996</td>\n",
       "      <td>0.487937</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.065132</td>\n",
       "      <td>-0.759238</td>\n",
       "      <td>-0.331773</td>\n",
       "      <td>-0.974439</td>\n",
       "      <td>-0.652712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.941535</td>\n",
       "      <td>-0.751594</td>\n",
       "      <td>-0.678840</td>\n",
       "      <td>-0.413494</td>\n",
       "      <td>-0.759399</td>\n",
       "      <td>-0.800922</td>\n",
       "      <td>0.483282</td>\n",
       "      <td>0.024035</td>\n",
       "      <td>-0.059681</td>\n",
       "      <td>0.556331</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.882483</td>\n",
       "      <td>-0.998027</td>\n",
       "      <td>-0.894743</td>\n",
       "      <td>0.490936</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.065751</td>\n",
       "      <td>-0.733542</td>\n",
       "      <td>-0.309363</td>\n",
       "      <td>-0.854674</td>\n",
       "      <td>-0.554066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95995</th>\n",
       "      <td>-0.807155</td>\n",
       "      <td>-0.826169</td>\n",
       "      <td>-0.662250</td>\n",
       "      <td>-0.393898</td>\n",
       "      <td>-0.698864</td>\n",
       "      <td>-0.796562</td>\n",
       "      <td>0.565474</td>\n",
       "      <td>0.047823</td>\n",
       "      <td>-0.051387</td>\n",
       "      <td>0.603036</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.782493</td>\n",
       "      <td>-0.950175</td>\n",
       "      <td>-0.988489</td>\n",
       "      <td>0.486800</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.025898</td>\n",
       "      <td>-0.572245</td>\n",
       "      <td>-0.337691</td>\n",
       "      <td>-0.818247</td>\n",
       "      <td>-0.410921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95996</th>\n",
       "      <td>-0.933014</td>\n",
       "      <td>-0.823786</td>\n",
       "      <td>-0.681032</td>\n",
       "      <td>-0.440621</td>\n",
       "      <td>-0.716846</td>\n",
       "      <td>-0.796464</td>\n",
       "      <td>0.519907</td>\n",
       "      <td>0.082246</td>\n",
       "      <td>-0.068685</td>\n",
       "      <td>0.553613</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.925051</td>\n",
       "      <td>-0.981724</td>\n",
       "      <td>-0.996235</td>\n",
       "      <td>0.488039</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.063647</td>\n",
       "      <td>-0.537230</td>\n",
       "      <td>-0.340467</td>\n",
       "      <td>-0.975905</td>\n",
       "      <td>-0.392990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95997</th>\n",
       "      <td>-0.954136</td>\n",
       "      <td>-0.834444</td>\n",
       "      <td>-0.671945</td>\n",
       "      <td>-0.425892</td>\n",
       "      <td>-0.751976</td>\n",
       "      <td>-0.799258</td>\n",
       "      <td>0.512260</td>\n",
       "      <td>0.122872</td>\n",
       "      <td>-0.065661</td>\n",
       "      <td>0.556760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.948976</td>\n",
       "      <td>-0.987019</td>\n",
       "      <td>-0.977440</td>\n",
       "      <td>0.489600</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.065852</td>\n",
       "      <td>-0.555274</td>\n",
       "      <td>-0.322275</td>\n",
       "      <td>-0.955225</td>\n",
       "      <td>-0.571418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95998</th>\n",
       "      <td>-0.910498</td>\n",
       "      <td>-0.825632</td>\n",
       "      <td>-0.674738</td>\n",
       "      <td>-0.403593</td>\n",
       "      <td>-0.746701</td>\n",
       "      <td>-0.799408</td>\n",
       "      <td>0.528059</td>\n",
       "      <td>0.082246</td>\n",
       "      <td>-0.068820</td>\n",
       "      <td>0.561407</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.899548</td>\n",
       "      <td>-0.976080</td>\n",
       "      <td>-0.990519</td>\n",
       "      <td>0.489600</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.065852</td>\n",
       "      <td>-0.566054</td>\n",
       "      <td>-0.330801</td>\n",
       "      <td>-0.903574</td>\n",
       "      <td>-0.500645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95999</th>\n",
       "      <td>-0.821743</td>\n",
       "      <td>-0.821501</td>\n",
       "      <td>-0.666206</td>\n",
       "      <td>-0.404962</td>\n",
       "      <td>-0.731484</td>\n",
       "      <td>-0.780711</td>\n",
       "      <td>0.560192</td>\n",
       "      <td>0.047452</td>\n",
       "      <td>-0.069507</td>\n",
       "      <td>0.481329</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.799018</td>\n",
       "      <td>-0.953832</td>\n",
       "      <td>-0.997219</td>\n",
       "      <td>0.465177</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.039551</td>\n",
       "      <td>0.159564</td>\n",
       "      <td>-0.330801</td>\n",
       "      <td>-0.946321</td>\n",
       "      <td>-0.081259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96000 rows Ã— 912 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         maxret   sgr*tbl  invest*svar  cashpr*tms  pchcurrat*bm_sp  \\\n",
       "0     -0.952404 -0.792172    -0.680108   -0.408892        -0.720001   \n",
       "1     -0.948345 -0.862089    -0.681235   -0.412348        -0.753810   \n",
       "2     -0.968391 -0.805653    -0.677884   -0.399741        -0.772443   \n",
       "3     -0.923763 -0.773741    -0.672744   -0.412169        -0.761726   \n",
       "4     -0.941535 -0.751594    -0.678840   -0.413494        -0.759399   \n",
       "...         ...       ...          ...         ...              ...   \n",
       "95995 -0.807155 -0.826169    -0.662250   -0.393898        -0.698864   \n",
       "95996 -0.933014 -0.823786    -0.681032   -0.440621        -0.716846   \n",
       "95997 -0.954136 -0.834444    -0.671945   -0.425892        -0.751976   \n",
       "95998 -0.910498 -0.825632    -0.674738   -0.403593        -0.746701   \n",
       "95999 -0.821743 -0.821501    -0.666206   -0.404962        -0.731484   \n",
       "\n",
       "       pchdepr*svar  maxret*ntis  absacc*ntis  cashdebt*dp_sp  roic*tms  ...  \\\n",
       "0         -0.800710     0.484978     0.023247       -0.056024  0.558184  ...   \n",
       "1         -0.800555     0.484345     0.026845       -0.068120  0.552080  ...   \n",
       "2         -0.801024     0.487473     0.032162       -0.057052  0.558757  ...   \n",
       "3         -0.801338     0.480509    -0.003747       -0.063439  0.552081  ...   \n",
       "4         -0.800922     0.483282     0.024035       -0.059681  0.556331  ...   \n",
       "...             ...          ...          ...             ...       ...  ...   \n",
       "95995     -0.796562     0.565474     0.047823       -0.051387  0.603036  ...   \n",
       "95996     -0.796464     0.519907     0.082246       -0.068685  0.553613  ...   \n",
       "95997     -0.799258     0.512260     0.122872       -0.065661  0.556760  ...   \n",
       "95998     -0.799408     0.528059     0.082246       -0.068820  0.561407  ...   \n",
       "95999     -0.780711     0.560192     0.047452       -0.069507  0.481329  ...   \n",
       "\n",
       "       maxret*ep_sp  maxret*svar  salecash*tbl  pchsale_pchinvt*svar  sic_60  \\\n",
       "0         -0.903560    -0.998373     -0.994198              0.487698    -1.0   \n",
       "1         -0.895688    -0.998244     -0.994507              0.492519    -1.0   \n",
       "2         -0.934565    -0.998882     -0.856003              0.488916    -1.0   \n",
       "3         -0.848018    -0.997461     -0.992996              0.487937    -1.0   \n",
       "4         -0.882483    -0.998027     -0.894743              0.490936    -1.0   \n",
       "...             ...          ...           ...                   ...     ...   \n",
       "95995     -0.782493    -0.950175     -0.988489              0.486800    -1.0   \n",
       "95996     -0.925051    -0.981724     -0.996235              0.488039     1.0   \n",
       "95997     -0.948976    -0.987019     -0.977440              0.489600    -1.0   \n",
       "95998     -0.899548    -0.976080     -0.990519              0.489600    -1.0   \n",
       "95999     -0.799018    -0.953832     -0.997219              0.465177    -1.0   \n",
       "\n",
       "       chinv*svar  idiovol*dp_sp  pchsale_pchxsga*tms  herf*dp_sp  tang*dp_sp  \n",
       "0       -0.063661      -0.762035            -0.322091   -0.978324   -0.616306  \n",
       "1       -0.066832      -0.866056            -0.324468   -0.783369   -0.325258  \n",
       "2       -0.060345      -0.830636            -0.323207   -0.384286   -0.436603  \n",
       "3       -0.065132      -0.759238            -0.331773   -0.974439   -0.652712  \n",
       "4       -0.065751      -0.733542            -0.309363   -0.854674   -0.554066  \n",
       "...           ...            ...                  ...         ...         ...  \n",
       "95995   -0.025898      -0.572245            -0.337691   -0.818247   -0.410921  \n",
       "95996   -0.063647      -0.537230            -0.340467   -0.975905   -0.392990  \n",
       "95997   -0.065852      -0.555274            -0.322275   -0.955225   -0.571418  \n",
       "95998   -0.065852      -0.566054            -0.330801   -0.903574   -0.500645  \n",
       "95999   -0.039551       0.159564            -0.330801   -0.946321   -0.081259  \n",
       "\n",
       "[96000 rows x 912 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7a4aeba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a92d734",
   "metadata": {},
   "outputs": [],
   "source": [
    "del([data_ch,data_ch_top,data_ch_bot,data_ch_d,data_ch_top_d,data_ch_bot_d])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd4490a",
   "metadata": {},
   "source": [
    "## Model Fitting\n",
    "\n",
    "In total, 8 models, including ordinary least squares (__OLS__), partial least squares (__PLS__), principal component regression (__PCR__), elastic net (__ENet__), generalized linear model with group lasso (__GLM__), random forest (__RF__), gradient boosting regression trees (__GBRT__) and neural networks (__NN__), are implemented in this paper. The objective loss functions are mean squared errors and Huber robust objective function that substitute squared loss with absolute loss for outliers. The training process is done in the training set, and the hyperparameters are tuned in the validation set.\n",
    "\n",
    "Due to the limitation of computational power, I mainly use the default or preselected hyperparameters when training the model. However, as the target of this replication is to get familiar with the whole process of implementing machine learning methods for empirical asset pricing research, I will conduct tuning process for some simple models (e.g., tuning the parameter $\\xi$ in Huber loss function when using preselected features including size, bm and momentum). Besides, as scikit-learn does not provide the flexibility to customize loss function in the training process, I will also write a customized code to incorporate Huber loss with elastic net."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51eaaa1",
   "metadata": {},
   "source": [
    "### Customized Loss Function, Scoring Functions, Validation Funtion and Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff8f3ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Loss Function\n",
    "# Huber objective function\n",
    "def huber(actual, predicted, xi):\n",
    "    actual, predicted = np.array(actual).flatten(), np.array(predicted).flatten()\n",
    "    resid = actual - predicted\n",
    "    huber_loss = np.where(np.abs(resid)<=xi, diff**2, 2*xi*np.abs(resid)-xi**2)\n",
    "    return np.mean(huber_loss)\n",
    "\n",
    "# Scoring Function\n",
    "# out-of-sample R squared\n",
    "def R_oos(actual, predicted):\n",
    "    actual, predicted = np.array(actual), np.array(predicted).flatten()\n",
    "    predicted = np.where(predicted<0,0,predicted)\n",
    "    return 1 - (np.dot((actual-predicted),(actual-predicted)))/(np.dot(actual,actual))\n",
    "\n",
    "# Validation Function\n",
    "def val_fun(model, params: dict, X_trn, y_trn, X_vld, y_vld, illustration=True, sleep=0, is_NN=False):\n",
    "    best_ros = None\n",
    "    lst_params = list(ParameterGrid(params))\n",
    "    for param in lst_params:\n",
    "        if best_ros == None:\n",
    "            if is_NN:\n",
    "                mod = model().set_params(**param).fit(X_trn, y_trn, X_vld, y_vld)\n",
    "            else:\n",
    "                mod = model().set_params(**param).fit(X_trn, y_trn)\n",
    "            best_mod = mod\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            best_ros = R_oos(y_vld, y_pred)\n",
    "            best_param = param\n",
    "            if illustration:\n",
    "                print(f'Model with params: {param} finished.')\n",
    "                print(f'with out-of-sample R squared on validation set: {best_ros*100:.5f}%')\n",
    "                print('*'*60)\n",
    "        else:\n",
    "            time.sleep(sleep)\n",
    "            if is_NN:\n",
    "                mod = model().set_params(**param).fit(X_trn, y_trn, X_vld, y_vld)\n",
    "            else:\n",
    "                mod = model().set_params(**param).fit(X_trn, y_trn)\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            ros = R_oos(y_vld, y_pred)\n",
    "            if illustration:\n",
    "                print(f'Model with params: {param} finished.')\n",
    "                print(f'with out-of-sample R squared on validation set: {ros*100:.5f}%')\n",
    "                print('*'*60)\n",
    "            if ros > best_ros:\n",
    "                best_ros = ros\n",
    "                best_mod = mod\n",
    "                best_param = param\n",
    "    if illustration:\n",
    "        print('\\n'+'#'*60)\n",
    "        print('Tuning process finished!!!')\n",
    "        print(f'The best setting is: {best_param}')\n",
    "        print(f'with R2oos {best_ros*100:.2f}% on validation set.')\n",
    "        print('#'*60)\n",
    "    return best_mod\n",
    "    \n",
    "\n",
    "# Pairwise Comparison\n",
    "# Diebold-Mariano test statistics\n",
    "\n",
    "# Evaluation Output\n",
    "def evaluate(actual, predicted, insample=False):\n",
    "    if insample:\n",
    "        print('*'*15+'In-Sample Metrics'+'*'*15)\n",
    "        print(f'The in-sample R2 is {r2_score(actual,predicted)*100:.2f}%')\n",
    "        print(f'The in-sample MSE is {mean_squared_error(actual,predicted):.3f}')\n",
    "    else:\n",
    "        print('*'*15+'Out-of-Sample Metrics'+'*'*15)\n",
    "        print(f'The out-of-sample R2 is {R_oos(actual,predicted)*100:.2f}%')\n",
    "        print(f'The out-of-sample MSE is {mean_squared_error(actual,predicted):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb531950",
   "metadata": {},
   "source": [
    "### OLS\n",
    "#### MSE as objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b98d30c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 19.40%\n",
      "The in-sample MSE is 0.009\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is -88.69%\n",
      "The out-of-sample MSE is 0.053\n",
      "CPU times: user 18.4 s, sys: 2.97 s, total: 21.3 s\n",
      "Wall time: 14.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "time.sleep(10)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# OLS with all features\n",
    "OLS = LinearRegression().fit(X_trn,y_trn)\n",
    "evaluate(y_trn, OLS.predict(X_trn), insample=True)\n",
    "evaluate(y_tst, OLS.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d331e058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 0.45%\n",
      "The in-sample MSE is 0.011\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.45%\n",
      "The out-of-sample MSE is 0.009\n",
      "CPU times: user 58.4 ms, sys: 11.2 ms, total: 69.6 ms\n",
      "Wall time: 15.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# OLS with preselected size, bm, and momentum covariates\n",
    "features_3 = ['mvel1','bm','mom1m','mom6m','mom12m','mom36m']\n",
    "OLS_3 = LinearRegression().fit(X_trn[features_3],y_trn)\n",
    "evaluate(y_trn, OLS_3.predict(X_trn[features_3]), insample=True)\n",
    "evaluate(y_tst, OLS_3.predict(X_tst[features_3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bc6c5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a22e31",
   "metadata": {},
   "source": [
    "#### Huber objective fuction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b14b019",
   "metadata": {},
   "source": [
    "The authors choose $\\xi$ as 99.9% percentile of the pricing error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1942c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 13.48%\n",
      "The in-sample MSE is 0.010\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is -3.05%\n",
      "The out-of-sample MSE is 0.010\n",
      "CPU times: user 3min 5s, sys: 38.9 s, total: 3min 44s\n",
      "Wall time: 48.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "time.sleep(10)\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "# OLS by Huber robust objective function with all features\n",
    "epsilon = np.max(((y_trn-OLS.predict(X_trn)).quantile(.999),1))\n",
    "OLS_H = HuberRegressor(epsilon=epsilon).fit(X_trn,y_trn)\n",
    "evaluate(y_trn, OLS_H.predict(X_trn), insample=True)\n",
    "evaluate(y_tst, OLS_H.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dae7ff21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 0.23%\n",
      "The in-sample MSE is 0.011\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.61%\n",
      "The out-of-sample MSE is 0.009\n",
      "CPU times: user 1.22 s, sys: 237 ms, total: 1.46 s\n",
      "Wall time: 267 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "# OLS by Huber robust objective function\n",
    "# with preselected size, bm, and momentum covariates\n",
    "epsilon = np.max(((y_trn-OLS_3.predict(X_trn[features_3])).quantile(.999),1))\n",
    "features_3 = ['mvel1','bm','mom1m','mom6m','mom12m','mom36m']\n",
    "OLS_H_3 = HuberRegressor(epsilon=epsilon).fit(X_trn[features_3],y_trn)\n",
    "\n",
    "evaluate(y_trn, OLS_H_3.predict(X_trn[features_3]), insample=True)\n",
    "evaluate(y_tst, OLS_H_3.predict(X_tst[features_3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a67e490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e42975a",
   "metadata": {},
   "source": [
    "### PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a7de9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'n_components': 1} finished.\n",
      "with out-of-sample R squared on validation set: 0.00040%\n",
      "************************************************************\n",
      "Model with params: {'n_components': 5} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'n_components': 10} finished.\n",
      "with out-of-sample R squared on validation set: -0.00385%\n",
      "************************************************************\n",
      "Model with params: {'n_components': 50} finished.\n",
      "with out-of-sample R squared on validation set: -1927.66308%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'n_components': 1}\n",
      "with R2oos 0.00% on validation set.\n",
      "############################################################\n",
      "CPU times: user 2min 27s, sys: 22.5 s, total: 2min 49s\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "params = {'n_components': [1, 5, 10, 50]}\n",
    "PLS = val_fun(PLSRegression,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c989fa3",
   "metadata": {},
   "source": [
    "As the result suggests, let's use only 1 component in PLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a7e461a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 5.70%\n",
      "The in-sample MSE is 0.011\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.02%\n",
      "The out-of-sample MSE is 0.009\n",
      "CPU times: user 1.57 s, sys: 668 ms, total: 2.24 s\n",
      "Wall time: 492 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pls_pred_is = PLS.predict(X_trn)\n",
    "pls_pred_os = PLS.predict(X_tst)\n",
    "evaluate(y_trn, pls_pred_is, insample=True) \n",
    "evaluate(y_tst, pls_pred_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0cafc1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b874147",
   "metadata": {},
   "source": [
    "### PCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b17e344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, HuberRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class PCRegressor:\n",
    "    \n",
    "    def __init__(self,n_PCs=1,loss='mse'):\n",
    "        self.n_PCs = n_PCs\n",
    "        if loss not in ['huber','mse']:\n",
    "            raise AttributeError(\n",
    "            f\"The loss should be either 'huber' or 'mse', but {loss} is given\"\n",
    "            )\n",
    "        else:\n",
    "            self.loss = loss\n",
    "        \n",
    "    def set_params(self, **params):\n",
    "        for param in params.keys():\n",
    "            setattr(self, param, params[param])\n",
    "        return self\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        X = np.array(X)\n",
    "        N,K = X.shape\n",
    "        y = np.array(y_trn).reshape((N,1))\n",
    "        self.mu = np.mean(X,axis=0).reshape((1,K))\n",
    "        self.sigma = np.std(X,axis=0).reshape((1,K))\n",
    "        self.sigma = np.where(self.sigma==0,1,self.sigma)\n",
    "        X = (X-self.mu)/self.sigma\n",
    "        pca = PCA()\n",
    "        X = pca.fit_transform(X)[:,:self.n_PCs]\n",
    "        self.pc_coef = pca.components_.T[:,:self.n_PCs]\n",
    "        if self.loss == 'mse':\n",
    "            self.model = LinearRegression().fit(X,y)\n",
    "        else:\n",
    "            self.model = HuberRegressor().fit(X,y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        X = np.array(X)\n",
    "        X = (X-self.mu)/self.sigma\n",
    "        X = X @ self.pc_coef\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85c33525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'loss': 'mse', 'n_PCs': 1} finished.\n",
      "with out-of-sample R squared on validation set: 0.06330%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 3} finished.\n",
      "with out-of-sample R squared on validation set: 0.03821%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 5} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 7} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 10} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 50} finished.\n",
      "with out-of-sample R squared on validation set: 0.62278%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 1} finished.\n",
      "with out-of-sample R squared on validation set: 0.05717%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 3} finished.\n",
      "with out-of-sample R squared on validation set: 0.03226%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 5} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 7} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 10} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 50} finished.\n",
      "with out-of-sample R squared on validation set: 1.94984%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'loss': 'huber', 'n_PCs': 50}\n",
      "with R2oos 1.95% on validation set.\n",
      "############################################################\n",
      "CPU times: user 9min 11s, sys: 1min 19s, total: 10min 30s\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# principal component regression\n",
    "params = {'n_PCs':[1,3,5,7,10,50],'loss':['mse','huber']}\n",
    "PCR = val_fun(PCRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,sleep=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a521476c",
   "metadata": {},
   "source": [
    "As the result suggests, let's use 3 principal components in PCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b074eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c710cbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 6.88%\n",
      "The in-sample MSE is 0.011\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is -0.01%\n",
      "The out-of-sample MSE is 0.010\n",
      "CPU times: user 2.25 s, sys: 1.57 s, total: 3.82 s\n",
      "Wall time: 2.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(y_trn, PCR.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, PCR.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c80a9a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbfb907",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f01ea",
   "metadata": {},
   "source": [
    "Since scikit-learn does not provide the flexibility of choosing objective function, I code this part refering to the accelerated proximal algorithm (APG) documented in the online appendix. Besides, I see lasso and ridge as a special case of elastic net.\n",
    "\n",
    "I use the setting from the paper, where the regularized objective function is:\n",
    "$$L(\\theta;\\cdot) = L(\\theta) + \\phi(\\theta;\\cdot)$$\n",
    "where $\\theta$ is the vector of parameters, $L(\\theta)$ is the loss funciton (MSE or Huber loss in our case), and $\\phi(\\theta;\\cdot)$ is the penalty.\n",
    "\n",
    "Specifically, for our elastic net:\n",
    "$$\\phi(\\theta;\\lambda,\\rho)=\\lambda(1-\\rho)\\sum_{j=1}^{P}|\\theta_{j}|+\\frac{1}{2}\\lambda\\rho\\sum_{j=1}^{P}\\theta_{j}^{2}$$\n",
    "\n",
    "As for the details of APG, I refer the readers to the online appendix of the paper, which can be found on Prof Xiu's [website](https://dachxiu.chicagobooth.edu/). The parameter $\\gamma$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98d17530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse\n",
    "def mse(actual, predicted):\n",
    "    actual, predicted = np.array(actual).flatten(), np.array(predicted).flatten()\n",
    "    resid = actual - predicted\n",
    "    return np.mean(resid**2)\n",
    "\n",
    "# huber objective function\n",
    "def huber(actual, predicted, xi):\n",
    "    actual, predicted = np.array(actual).flatten(), np.array(predicted).flatten()\n",
    "    resid = actual - predicted\n",
    "    huber_loss = np.where(np.abs(resid)<=xi, diff**2, 2*xi*np.abs(resid)-xi**2)\n",
    "    return np.mean(huber_loss)\n",
    "\n",
    "# gradient of mse\n",
    "def grad_mse(X, y, theta):\n",
    "    K = X.shape[1]\n",
    "    X = np.array(X)\n",
    "    N = len(y)\n",
    "    y = np.array(y).reshape((N,1))\n",
    "    theta = np.array(theta).reshape((K,1))\n",
    "    return (X.T @ (y - X@theta))/N\n",
    "\n",
    "# gradient of huber loss\n",
    "def grad_huber(X, y, theta, xi):\n",
    "    K = X.shape[1]\n",
    "    X = np.array(X)\n",
    "    N = len(y)\n",
    "    y = np.array(y).reshape((N,1))\n",
    "    theta = np.array(theta).reshape((K,1))\n",
    "    resid = y - X@theta\n",
    "    ind_m = np.where(np.abs(resid)<=xi)\n",
    "    ind_u = np.where(resid>xi)\n",
    "    ind_l = np.where(resid< -xi)\n",
    "    try:\n",
    "        grad_m = X[ind_m].T @ (y[ind_m] - X[ind_m]@theta)\n",
    "    except:\n",
    "        grad_m = np.zeros((K,1))\n",
    "    try:\n",
    "        grad_u = 2*xi* X[ind_u].T@np.ones((len(ind_u[0]),1))\n",
    "    except:\n",
    "        grad_u = np.zeros((K,1))\n",
    "    try:\n",
    "        grad_l = -2*xi* X[ind_l].T@np.ones((len(ind_l[0]),1))\n",
    "    except:\n",
    "        grad_l = np.zeros((K,1))\n",
    "    return (grad_m+grad_u+grad_l)/N\n",
    "\n",
    "# proximal operator\n",
    "def prox(theta,lmd,rho,gamma):\n",
    "    return (1/(1+lmd*gamma*rho))*softhred(theta,(1-rho)*gamma*lmd)\n",
    "\n",
    "# soft-thresholding operator\n",
    "def softhred(x,mu):\n",
    "    x = np.where(np.abs(x)<=mu, 0, x)\n",
    "    x = np.where((np.abs(x)>mu) & (x>0), x-mu, x)\n",
    "    x = np.where((np.abs(x)>mu) & (x<0), x+mu, x)\n",
    "    return x\n",
    "\n",
    "# Elastic Net\n",
    "class ENet:\n",
    "    \n",
    "    def __init__(\n",
    "        self, lmd=1, rho=0.5, L=1, verbose=False,\n",
    "        xi=1.35, max_iter=3000, tol=1e-4, loss='huber', random_state=None\n",
    "    ):\n",
    "        self.lmd = lmd\n",
    "        self.rho = rho\n",
    "        self.L = L\n",
    "        self.verbose = verbose\n",
    "        self.xi = xi\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        if loss not in ['huber','mse']:\n",
    "            raise AttributeError(\n",
    "            f\"The loss should be either 'huber' or 'mse', but {loss} is given\"\n",
    "            )\n",
    "        else:\n",
    "            self.loss = loss\n",
    "            \n",
    "    def set_params(self, **params):\n",
    "        for param in params.keys():\n",
    "            setattr(self, param, params[param])\n",
    "        return self\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        K = X.shape[1]\n",
    "        X = np.array(X)\n",
    "        N = len(y)\n",
    "        y = np.array(y).reshape((N,1))\n",
    "        gamma = 1/self.L\n",
    "        # initialize theta\n",
    "        if self.random_state != None:\n",
    "            np.random.seed(self.random_state)\n",
    "            theta = np.random.uniform(size=(K,1))\n",
    "        else:\n",
    "            theta = np.zeros((K,1))\n",
    "        \n",
    "        for m in np.arange(self.max_iter):\n",
    "            theta_old = theta\n",
    "            \n",
    "            if self.loss == 'mse':\n",
    "                theta_bar = theta - gamma*grad_mse(X,y,theta)\n",
    "            else:\n",
    "                theta_bar = theta - gamma*grad_huber(X,y,theta,self.xi)\n",
    "                \n",
    "            theta_til = prox(theta_bar,self.lmd,self.rho,gamma)\n",
    "            theta = theta_til + m/(m+3)*(theta_til-theta)\n",
    "            gamma = gamma\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f'{m+1} iters finished.')\n",
    "                print(f'theta = {theta.T}')\n",
    "                print(f'{np.sum((theta-theta_old)**2)}')\n",
    "            \n",
    "            if np.sum((theta-theta_old)**2)<np.sum(theta_old**2*self.tol) or np.sum(np.abs(theta-theta_old))==0:\n",
    "                break\n",
    "        self.theta = theta_old\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        return X@self.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc9cae43",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'L': 43030456.772059955, 'lmd': 0.0001, 'loss': 'mse'} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'L': 43030456.772059955, 'lmd': 0.1, 'loss': 'mse'} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'L': 43030456.772059955, 'lmd': 0.0001, 'loss': 'mse'}\n",
      "with R2oos 0.00% on validation set.\n",
      "############################################################\n",
      "CPU times: user 2min 37s, sys: 31 s, total: 3min 8s\n",
      "Wall time: 27.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from numpy.linalg import svd\n",
    "\n",
    "# compute L\n",
    "L = np.max(svd(X_trn,compute_uv=False))**2\n",
    "\n",
    "params = {\n",
    "    'lmd':[1e-4,.1],\n",
    "    'L':[L],\n",
    "    'loss':['mse']\n",
    "}\n",
    "\n",
    "EN_my_mse = val_fun(ENet,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c32292bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is -0.03%\n",
      "The in-sample MSE is 0.012\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.00%\n",
      "The out-of-sample MSE is 0.009\n",
      "CPU times: user 1.41 s, sys: 494 ms, total: 1.91 s\n",
      "Wall time: 366 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(y_trn, EN_my_mse.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, EN_my_mse.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9900438d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd0cb0f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'L': 2.0, 'lmd': 0.0001, 'loss': 'huber'} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'L': 2.0, 'lmd': 0.00019999999999999998, 'loss': 'huber'} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'L': 2.0, 'lmd': 0.0003, 'loss': 'huber'} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'L': 2.0, 'lmd': 0.00039999999999999996, 'loss': 'huber'} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'L': 2.0, 'lmd': 0.0005, 'loss': 'huber'} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'L': 2.0, 'lmd': 0.0006000000000000001, 'loss': 'huber'} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'L': 2.0, 'lmd': 0.0007, 'loss': 'huber'} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'L': 2.0, 'lmd': 0.0007999999999999999, 'loss': 'huber'} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'L': 2.0, 'lmd': 0.0009, 'loss': 'huber'} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'L': 2.0, 'lmd': 0.001, 'loss': 'huber'} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'L': 2.0, 'lmd': 0.0001, 'loss': 'huber'}\n",
      "with R2oos 0.00% on validation set.\n",
      "############################################################\n",
      "CPU times: user 20.8 s, sys: 8.01 s, total: 28.8 s\n",
      "Wall time: 6.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xi = np.max((1,(y_trn-EN_my_mse.predict(X_trn).flatten()).quantile(.999)))\n",
    "params = {\n",
    "    'lmd':list(np.linspace(1e-4,1e-3,10)),\n",
    "    'L':[2*xi],\n",
    "    #'L':[2],\n",
    "    'loss':['huber']\n",
    "}\n",
    "EN_my_hub = val_fun(ENet,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f4e2790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d245d50",
   "metadata": {},
   "source": [
    "However, the choice of learning rate $\\gamma$ in APG is quite cruicial. According to [Parikh and Boyd (2013)](https://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf), the algorithm converges when $\\gamma \\in (0, 1/L]$, where $L$ is the _Lipschitz_ constant of the gradient of the objective funtion. As for mse, L is the square of the largest singular value of the dataset. Though my code for APG seems to work, it returns weights with same value. The readers can check this by setting the parameter _verbose_ as _False_. Therefore, I first use the elastic net provided by sklearn for the elastic net with MSE as objective function. Then, I write the fitting process as an unconstraint optimization using the optimization package from scipy. However, as I use Nelder-Mead as solver, it takes much time to get the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3392e9f",
   "metadata": {},
   "source": [
    "__Elastic Net from sklearn__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "29f4eba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'alpha': 0.0001} finished.\n",
      "with out-of-sample R squared on validation set: -79.93287%\n",
      "************************************************************\n",
      "Model with params: {'alpha': 0.1} finished.\n",
      "with out-of-sample R squared on validation set: 0.62828%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'alpha': 0.1}\n",
      "with R2oos 0.63% on validation set.\n",
      "############################################################\n",
      "CPU times: user 2min 33s, sys: 4.74 s, total: 2min 38s\n",
      "Wall time: 33.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "time.sleep(10)\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "params = {'alpha':[1e-4,.1]}\n",
    "EN_sk = val_fun(ElasticNet,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d8363664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 0.00%\n",
      "The in-sample MSE is 0.012\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.35%\n",
      "The out-of-sample MSE is 0.009\n",
      "CPU times: user 245 ms, sys: 17.6 ms, total: 263 ms\n",
      "Wall time: 70.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(y_trn, EN_sk.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, EN_sk.predict(X_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5703965a",
   "metadata": {},
   "source": [
    "__Elastic Net with Huber Loss by Scipy__\n",
    "\n",
    "Next, I formulate this problem in the framework of scipy's optimization package. Though BFGS solver uses gradient and will be faster than Nelder-Mead, I encounter a runtime error _'Desired error not necessarily achieved due to precision loss'_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f8597bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from functools import partial\n",
    "\n",
    "# penalized mse\n",
    "def mse_pnl(theta, X, y, lmd, rho):\n",
    "    K = X.shape[1]\n",
    "    X = np.array(X)\n",
    "    N = len(y)\n",
    "    y = np.array(y).reshape((N,1))\n",
    "    resid = y - X@theta.reshape((K,1))\n",
    "    return np.mean(resid**2) + lmd*(1-rho)*np.sum(np.abs(theta)) + 0.5*lmd*rho*np.sum(theta**2)\n",
    "\n",
    "# penalized huber objective function\n",
    "def huber_pnl(theta, X, y, xi, lmd, rho):\n",
    "    K = X.shape[1]\n",
    "    X = np.array(X)\n",
    "    N = len(y)\n",
    "    y = np.array(y).reshape((N,1))\n",
    "    resid = y - X@theta.reshape((K,1))\n",
    "    huber_loss = np.where(np.abs(resid)<=xi, resid**2, 2*xi*np.abs(resid)-xi**2)\n",
    "    return np.mean(huber_loss) + lmd*(1-rho)*np.sum(np.abs(theta)) + 0.5*lmd*rho*np.sum(theta**2)\n",
    "\n",
    "# gradient of mse\n",
    "def grad_mse(theta, X, y, lmd, rho):\n",
    "    K = X.shape[1]\n",
    "    X = np.array(X)\n",
    "    N = len(y)\n",
    "    y = np.array(y).reshape((N,1))\n",
    "    theta = np.array(theta).reshape((K,1))\n",
    "    grad = (X.T @ (y - X@theta))/N + lmd*(1-rho)*np.where(theta>0,1,-1) + lmd*rho*theta\n",
    "    return grad.flatten()\n",
    "\n",
    "# gradient of huber loss\n",
    "def grad_huber(theta, X, y, xi, lmd, rho):\n",
    "    K = X.shape[1]\n",
    "    X = np.array(X)\n",
    "    N = len(y)\n",
    "    y = np.array(y).reshape((N,1))\n",
    "    theta = np.array(theta).reshape((K,1))\n",
    "    resid = y - X@theta\n",
    "    ind_m = np.where(np.abs(resid)<=xi)\n",
    "    ind_u = np.where(resid>xi)\n",
    "    ind_l = np.where(resid< -xi)\n",
    "    try:\n",
    "        grad_m = X[ind_m].T @ (y[ind_m] - X[ind_m]@theta)\n",
    "    except:\n",
    "        grad_m = np.zeros((K,1))\n",
    "    try:\n",
    "        grad_u = 2*xi* X[ind_u].T@np.ones((len(ind_u[0]),1))\n",
    "    except:\n",
    "        grad_u = np.zeros((K,1))\n",
    "    try:\n",
    "        grad_l = -2*xi* X[ind_l].T@np.ones((len(ind_l[0]),1))\n",
    "    except:\n",
    "        grad_l = np.zeros((K,1))\n",
    "    grad = (grad_m+grad_u+grad_l)/N + lmd*(1-rho)*np.where(theta>0,1,-1) + lmd*rho*theta\n",
    "    return grad.flatten()\n",
    "\n",
    "# Elastic Net\n",
    "class ENet:\n",
    "    \n",
    "    def __init__(\n",
    "        self, lmd, rho, xi=1.35, loss='huber', random_state=None, fit_intercept=True\n",
    "    ):\n",
    "        self.lmd = lmd\n",
    "        self.rho = rho\n",
    "        self.xi = xi\n",
    "        self.random_state = random_state\n",
    "        self.fit_intercept = fit_intercept\n",
    "        if loss not in ['huber','mse']:\n",
    "            raise AttributeError(\n",
    "            f\"The loss should be either 'huber' or 'mse', but {loss} is given\"\n",
    "            )\n",
    "        else:\n",
    "            self.loss = loss\n",
    "            \n",
    "    def set_params(self, **params):\n",
    "        for param in params.keys():\n",
    "            setattr(self, param, params[param])\n",
    "        return self\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        K = X.shape[1]\n",
    "        X = np.array(X)\n",
    "        N = len(y)\n",
    "        y = np.array(y).reshape((N,1))\n",
    "        if self.fit_intercept:\n",
    "            K += 1\n",
    "            X = np.concatenate((np.ones((N,1)),X),axis=1)\n",
    "        # initialize theta\n",
    "        if self.random_state != None:\n",
    "            np.random.seed(self.random_state)\n",
    "            theta = np.random.uniform(K)\n",
    "        else:\n",
    "            theta = np.zeros(K)\n",
    "        \n",
    "        if self.loss == 'huber':\n",
    "            res = minimize(\n",
    "                partial(huber_pnl, X=X, y=y, xi=self.xi, lmd=self.lmd, rho=self.rho), theta,\n",
    "                method='nelder-mead',\n",
    "                #method='BFGS',\n",
    "                #jac = partial(grad_huber, X=X, y=y, xi=self.xi, lmd=self.lmd, rho=self.rho),\n",
    "                options = {'disp': True}\n",
    "            )\n",
    "        else:\n",
    "            res = minimize(\n",
    "                partial(mse_pnl, X=X, y=y, lmd=self.lmd, rho=self.rho), theta, \n",
    "                method='nelder-mead',\n",
    "                #method='BFGS',\n",
    "                #jac = partial(grad_mse, X=X, y=y, lmd=self.lmd, rho=self.rho),\n",
    "                options = {'disp': True}\n",
    "            )\n",
    "        \n",
    "        self.theta = res.x.reshape((K,1))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        N = X.shape[0]\n",
    "        if self.fit_intercept:\n",
    "            X = np.concatenate((np.ones((N,1)),X),axis=1)\n",
    "        return X@self.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b8c331",
   "metadata": {},
   "source": [
    "As the Nelder-Mead solver is really slow, I randomly select 50 features to try whther my code works. I can foresee that my code will be much slower than sklearn's elastic net. Besides, as the solvers are different and the stop criteria are different, the results might be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d0d8e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.011510\n",
      "         Iterations: 990\n",
      "         Function evaluations: 1427\n",
      "CPU times: user 24.5 s, sys: 614 ms, total: 25.2 s\n",
      "Wall time: 13.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "time.sleep(10)\n",
    "import random\n",
    "\n",
    "# randomly select 50 \n",
    "random.seed(12308)\n",
    "features_rdm = random.sample(list(X_trn.columns),50)\n",
    "\n",
    "# fit elastic net with mse using my code\n",
    "EN_my_mse_rdm = ENet(lmd=.01,rho=.5,loss='mse').fit(X_trn[features_rdm],y_trn)\n",
    "# the fitted coefficients\n",
    "EN_my_mse_rdm.theta.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c14f5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['idiovol*dfy', 'convind*bm_sp', 'secured*ntis', 'invest',\n",
       "       'stdacc*ep_sp', 'tang*dfy', 'orgcap*ep_sp', 'salecash*tbl',\n",
       "       'ps*svar', 'mvel1*tbl', 'sic_53', 'sp', 'bm', 'nincr*ntis',\n",
       "       'grcapx*ntis', 'mom12m*dp_sp', 'pchgm_pchsale', 'sic_29', 'sic_37',\n",
       "       'chtx*tms', 'pchcapx_ia*svar', 'agr*ntis', 'baspread*svar',\n",
       "       'pricedelay', 'divo*ep_sp', 'salerec*bm_sp', 'chempia*bm_sp',\n",
       "       'divi*svar', 'sp*ep_sp', 'sic_48', 'rd_mve*tbl', 'quick*ep_sp',\n",
       "       'chinv*tbl', 'rsup*dp_sp', 'salerec*tbl', 'mvel1*ep_sp',\n",
       "       'pricedelay*dfy', 'cash*bm_sp', 'agr*tms', 'mve_ia*tms',\n",
       "       'gma*bm_sp', 'sic_17', 'absacc', 'dy*tms', 'agr*bm_sp', 'cinvest',\n",
       "       'depr*ep_sp', 'rd_sale*dp_sp', 'tb*ep_sp', 'hire*tbl'],\n",
       "      dtype='<U15')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 50 features selected\n",
    "np.array(features_rdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ca556c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is -0.03%\n",
      "The in-sample MSE is 0.012\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.00%\n",
      "The out-of-sample MSE is 0.009\n",
      "CPU times: user 66.7 ms, sys: 14.6 ms, total: 81.3 ms\n",
      "Wall time: 23.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(y_trn, EN_my_mse_rdm.predict(X_trn[features_rdm]), insample=True) \n",
    "evaluate(y_tst, EN_my_mse_rdm.predict(X_tst[features_rdm]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e70ba3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 96 ms, sys: 11.8 ms, total: 108 ms\n",
      "Wall time: 28.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.0018227, -0.       , -0.       ,  0.       , -0.       ,\n",
       "       -0.       , -0.       ,  0.       ,  0.       , -0.       ,\n",
       "        0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "        0.       ,  0.       ,  0.       ,  0.       , -0.       ,\n",
       "        0.       ,  0.       , -0.       , -0.       ,  0.       ,\n",
       "        0.       ,  0.       , -0.       , -0.       ,  0.       ,\n",
       "       -0.       ,  0.       , -0.       ,  0.       ,  0.       ,\n",
       "        0.       ,  0.       , -0.       , -0.       ,  0.       ,\n",
       "       -0.       , -0.       , -0.       ,  0.       , -0.       ,\n",
       "        0.       , -0.       , -0.       , -0.       ,  0.       ,\n",
       "       -0.       ])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# fit elastic net with mse by sklearn\n",
    "EN_sk_rdm = ElasticNet(alpha=.01).fit(X_trn[features_rdm],y_trn)\n",
    "# fitted coefficients\n",
    "np.array([EN_sk_rdm.intercept_]+list(EN_sk_rdm.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fe2b3828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 0.00%\n",
      "The in-sample MSE is 0.012\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.35%\n",
      "The out-of-sample MSE is 0.009\n",
      "CPU times: user 41.6 ms, sys: 8.28 ms, total: 49.9 ms\n",
      "Wall time: 11.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(y_trn, EN_sk_rdm.predict(X_trn[features_rdm]), insample=True) \n",
    "evaluate(y_tst, EN_sk_rdm.predict(X_tst[features_rdm]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9864a4e9",
   "metadata": {},
   "source": [
    "Though I have foreseen that the results might be different between my code and sklearn, the difference is beyond my expectation. Anyway, let's try my code with huber loss. We should expect some improvement compared with mse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cc4bb15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.011509\n",
      "         Iterations: 972\n",
      "         Function evaluations: 1404\n",
      "CPU times: user 26.3 s, sys: 768 ms, total: 27.1 s\n",
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "time.sleep(10)\n",
    "\n",
    "# fit elastic net with huber loss using my code\n",
    "EN_my_hub_rdm = ENet(lmd=.01,rho=.5,loss='huber').fit(X_trn[features_rdm],y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c42e69e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is -0.03%\n",
      "The in-sample MSE is 0.012\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.00%\n",
      "The out-of-sample MSE is 0.009\n",
      "CPU times: user 45.8 ms, sys: 4.54 ms, total: 50.4 ms\n",
      "Wall time: 18 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(y_trn, EN_my_hub_rdm.predict(X_trn[features_rdm]), insample=True) \n",
    "evaluate(y_tst, EN_my_hub_rdm.predict(X_tst[features_rdm]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6874a33",
   "metadata": {},
   "source": [
    "Fortunately, there is indeed some improvement with regard to $R^{2}_{oos}$ as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025cb4a0",
   "metadata": {},
   "source": [
    "### GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aa1788aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from group_lasso import GroupLasso\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def SplineTransform(data,knots=3):\n",
    "    spline_data = pd.DataFrame(np.ones((data.shape[0],1)),index=data.index,columns=['const'])\n",
    "    for i in data.columns:\n",
    "        i_dat = data.loc[:,i]\n",
    "        i_sqr = i_dat**2\n",
    "        i_cut, bins = pd.cut(i_dat, 3, right=True, ordered=True, retbins=True)\n",
    "        i_dum = pd.get_dummies(i_cut)\n",
    "        for j in np.arange(knots):\n",
    "            i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
    "        i_dum.columns = [f\"{i}_{k}\" for k in np.arange(1,knots+1)]\n",
    "        spline_data = pd.concat((spline_data,i_dat,i_dum),axis=1)\n",
    "    return spline_data\n",
    "\n",
    "class GLMRegression:\n",
    "    \n",
    "    def __init__(self,knots=3,lmd=1e-4,l1_reg=1e-4,random_state=12308):\n",
    "        self.knots = knots\n",
    "        self.lmd = lmd\n",
    "        self.random_state = random_state\n",
    "        self.l1_reg = l1_reg\n",
    "        \n",
    "    def set_params(self, **params):\n",
    "        for param in params.keys():\n",
    "            setattr(self, param, params[param])\n",
    "        return self\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        groups = [0]+flatten([list(np.repeat(i,self.knots+1))[:] for i in np.arange(1,X.shape[1]+1)])\n",
    "        X = SplineTransform(X)\n",
    "        self.mod = GroupLasso(\n",
    "            groups=groups,group_reg=self.lmd,l1_reg=self.l1_reg,\n",
    "            fit_intercept=False,random_state=self.random_state\n",
    "        )\n",
    "        self.mod = self.mod.fit(X,y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        X = SplineTransform(X)\n",
    "        return self.mod.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fbf47d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'knots': 3, 'l1_reg': 0.0001, 'lmd': 0.0001} finished.\n",
      "with out-of-sample R squared on validation set: 0.91194%\n",
      "************************************************************\n",
      "Model with params: {'knots': 3, 'l1_reg': 0.0001, 'lmd': 0.1} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'knots': 3, 'l1_reg': 0, 'lmd': 0.0001} finished.\n",
      "with out-of-sample R squared on validation set: 0.86900%\n",
      "************************************************************\n",
      "Model with params: {'knots': 3, 'l1_reg': 0, 'lmd': 0.1} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'knots': 3, 'l1_reg': 0.0001, 'lmd': 0.0001}\n",
      "with R2oos 0.91% on validation set.\n",
      "############################################################\n",
      "CPU times: user 1h 1min 4s, sys: 47min 1s, total: 1h 48min 5s\n",
      "Wall time: 1h 12min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params = {\n",
    "    'knots':[3],\n",
    "    'lmd':[1e-4,1e-1],#list(np.linspace(1e-4,1e-1,10)),\n",
    "    'l1_reg':[1e-4,0]\n",
    "}\n",
    "GLM = val_fun(GLMRegression,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b6d760db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "474"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(GLM.mod.sparsity_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f7262cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "017a1035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 3.01%\n",
      "The in-sample MSE is 0.011\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.73%\n",
      "The out-of-sample MSE is 0.009\n",
      "CPU times: user 2min 19s, sys: 21.4 s, total: 2min 40s\n",
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(y_trn, GLM.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, GLM.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a74a0ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabced51",
   "metadata": {},
   "source": [
    "### Tree-based Models\n",
    "\n",
    "In this notebook, the best models are tree-based models, including LGBM, Random Forest and XGBoost. Quoted from the [first place solution](https://www.kaggle.com/competitions/ubiquant-market-prediction/discussion/338220) for [Ubiquant Market Prediction Competition](https://www.kaggle.com/competitions/ubiquant-market-prediction/overview): LGBM is a powerful model whose performance has been proven in many competitions. It was also the most stable (especially the consistency of CV and LB) and excellent in the experiment on the competition data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b95a29",
   "metadata": {},
   "source": [
    "### LightGBM\n",
    "\n",
    "From this section till XGBoost, we will explore some tree-based models. We skip the single decision tree, as it is highly sensitive to the dataset. We first try gradient boosting decision tree (GBDT). The rationale behind is that several weak learners together result in a better performance than a single sophisticated learner. I use LightGBM to accelerate the training process while reserve the accuracy.\n",
    "\n",
    "[LightGBM](https://lightgbm.readthedocs.io/en/v3.3.3/index.html) is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n",
    "\n",
    "- Faster training speed and higher efficiency.\n",
    "\n",
    "- Lower memory usage.\n",
    "\n",
    "- Better accuracy.\n",
    "\n",
    "- Support of parallel, distributed, and GPU learning.\n",
    "\n",
    "- Capable of handling large-scale data.\n",
    "\n",
    "LightGBM speeds up the training process of convenional gradient boosting decision tree by up to over 20 times while achieving almost the same accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a568a3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huber loss function for customized objective function\n",
    "\n",
    "# gradient of huber loss with respect to y_pred\n",
    "def grad_huber_obj(y_true, y_pred):\n",
    "    xi = 1.35 \n",
    "    # Though I do not want to make it hard-coded, lightgbm, behind the scene, evaluates the # of parameters\n",
    "    # of the objective function first, then pass according # of parameters. I tried to use partial to set \n",
    "    # the value of xi. It did not work.\n",
    "    # I refer the readers to the source code to have a better understanding of the issue:\n",
    "    # (https://github.com/microsoft/LightGBM/blob/master/python-package/lightgbm/sklearn.py)\n",
    "    y_true, y_pred = np.array(y_true).flatten(), np.array(y_pred).flatten()\n",
    "    N = len(y_true)\n",
    "    resid = y_true - y_pred\n",
    "    ind_m = np.where(np.abs(resid)<=xi)\n",
    "    ind_u = np.where(resid>xi)\n",
    "    ind_l = np.where(resid< -xi)\n",
    "    grad = np.zeros(N)\n",
    "    try:\n",
    "        grad[ind_m] = (-2*(y_true-y_pred))[ind_m]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        grad[ind_u] = np.repeat(2*xi,N)[ind_u]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        grad[ind_l] = np.repeat(-2*xi,N)[ind_l]\n",
    "    except:\n",
    "        pass\n",
    "    return grad/N\n",
    "\n",
    "# hessian of huber loss with respect to y_pred\n",
    "def hess_huber_obj(y_true, y_pred):\n",
    "    xi = 1.35\n",
    "    y_true, y_pred = np.array(y_true).flatten(), np.array(y_pred).flatten()\n",
    "    N = len(y_true)\n",
    "    resid = y_true - y_pred\n",
    "    ind_m = np.where(np.abs(resid)<=xi)\n",
    "    ind_u = np.where(resid>xi)\n",
    "    ind_l = np.where(resid< -xi)\n",
    "    hess = np.zeros(N)\n",
    "    try:\n",
    "        hess[ind_m] = np.repeat(2,N)[ind_m]\n",
    "    except:\n",
    "        pass\n",
    "    return hess/N\n",
    "\n",
    "# huber loss for lgbm\n",
    "def huber_obj(y_true, y_pred):\n",
    "    grad = grad_huber_obj(y_true, y_pred)\n",
    "    hess = hess_huber_obj(y_true, y_pred)\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3b40e07f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 10, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.87070%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 10, 'objective': <function huber_obj at 0x7f7e181e21f0>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.23432%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 50, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.92921%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 50, 'objective': <function huber_obj at 0x7f7e181e21f0>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.51671%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 100, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.09914%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 100, 'objective': <function huber_obj at 0x7f7e181e21f0>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.01141%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 200, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.34051%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 200, 'objective': <function huber_obj at 0x7f7e181e21f0>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.33230%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.55954%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500, 'objective': <function huber_obj at 0x7f7e181e21f0>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.50883%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.05162%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1000, 'objective': <function huber_obj at 0x7f7e181e21f0>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.09846%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 10, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.57218%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 10, 'objective': <function huber_obj at 0x7f7e181e21f0>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.26757%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 50, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.93968%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 50, 'objective': <function huber_obj at 0x7f7e181e21f0>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.81853%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 100, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.17236%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 100, 'objective': <function huber_obj at 0x7f7e181e21f0>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.10436%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 200, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.45845%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 200, 'objective': <function huber_obj at 0x7f7e181e21f0>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.44892%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.50675%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 500, 'objective': <function huber_obj at 0x7f7e181e21f0>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.63858%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.44237%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1000, 'objective': <function huber_obj at 0x7f7e181e21f0>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.57868%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 10, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.11745%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 10, 'objective': <function huber_obj at 0x7f7e181e21f0>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.03772%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 50, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.54282%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 50, 'objective': <function huber_obj at 0x7f7e181e21f0>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.50259%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 100, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.88436%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 100, 'objective': <function huber_obj at 0x7f7e181e21f0>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.89676%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 200, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.43950%\n",
      "************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 200, 'objective': <function huber_obj at 0x7f7e181e21f0>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.31480%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.36812%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500, 'objective': <function huber_obj at 0x7f7e181e21f0>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.43165%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.57602%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1000, 'objective': <function huber_obj at 0x7f7e181e21f0>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.72761%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 10, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.22861%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 10, 'objective': <function huber_obj at 0x7f7e181e21f0>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.12283%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.74927%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50, 'objective': <function huber_obj at 0x7f7e181e21f0>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.81013%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.82438%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100, 'objective': <function huber_obj at 0x7f7e181e21f0>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.38928%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 200, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.13666%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 200, 'objective': <function huber_obj at 0x7f7e181e21f0>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.01573%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -1.35126%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 500, 'objective': <function huber_obj at 0x7f7e181e21f0>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.74914%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -13.44124%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1000, 'objective': <function huber_obj at 0x7f7e181e21f0>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.87520%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100, 'objective': None, 'random_state': 12308}\n",
      "with R2oos 2.82% on validation set.\n",
      "############################################################\n",
      "CPU times: user 42min 58s, sys: 3min 36s, total: 46min 34s\n",
      "Wall time: 8min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "params = {\n",
    "    'objective':[None, huber_obj],\n",
    "    'max_depth':[1,2],\n",
    "    'n_estimators':[10,50,100,200,500,1000],\n",
    "    'random_state':[12308],\n",
    "    'learning_rate':[.01,.1]\n",
    "}\n",
    "LGBM = val_fun(LGBMRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "33a07592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "649"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd4087b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 19.41%\n",
      "The in-sample MSE is 0.009\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 1.26%\n",
      "The out-of-sample MSE is 0.009\n",
      "CPU times: user 2.29 s, sys: 898 ms, total: 3.19 s\n",
      "Wall time: 944 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(y_trn, LGBM.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, LGBM.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "18370f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c47f15",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b3106c80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'max_depth': 3, 'max_features': 30, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.00153%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 3, 'max_features': 50, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.30646%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 3, 'max_features': 100, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.47756%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 6, 'max_features': 30, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.01128%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 6, 'max_features': 50, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.85949%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 6, 'max_features': 100, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.21410%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'max_depth': 6, 'max_features': 100, 'n_estimators': 300, 'random_state': 12308}\n",
      "with R2oos 2.21% on validation set.\n",
      "############################################################\n",
      "CPU times: user 31min 54s, sys: 4.86 s, total: 31min 59s\n",
      "Wall time: 31min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [3, 6],\n",
    "    'max_features': [30, 50, 100],\n",
    "    'random_state': [12308]\n",
    "}\n",
    "RF = val_fun(RandomForestRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "21baaaa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "381e8488",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 19.96%\n",
      "The in-sample MSE is 0.009\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 2.29%\n",
      "The out-of-sample MSE is 0.008\n",
      "CPU times: user 1.86 s, sys: 276 ms, total: 2.14 s\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "time.sleep(10)\n",
    "\n",
    "evaluate(y_trn, RF.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, RF.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "20c33e76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2231a7",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "Typically, the winner models of most Kaggle competitions are XGBoost, Neural Nets, or the ensemble model of these two. Therefore, I give XGBoost a try here, though it is not included in the paper. Like other additive tree-based models, XGBoost model often achieves higher accuracy than a single decision tree. However, it sacrifices the intrinsic interpretability of decision trees. Typically, it is easy to encounter overfitting issue when using XGBoost, especially in a low signal-to-noise ratio context. Therefore, here I try some simple settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "78ff535f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.50783%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 600, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.43211%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 800, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.33577%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1000, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.02993%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 500, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.59823%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 600, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.21747%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 800, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.08021%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1000, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.10506%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 500, 'random_state': 12308}\n",
      "with R2oos 2.60% on validation set.\n",
      "############################################################\n",
      "CPU times: user 4h 12min 3s, sys: 2min 21s, total: 4h 14min 24s\n",
      "Wall time: 33min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [500,600,800,1000],\n",
    "    'max_depth': [1,2],\n",
    "    'random_state': [12308],\n",
    "    'learning_rate': [.01]\n",
    "}\n",
    "XGB = val_fun(XGBRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1485d003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "006affbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 14.87%\n",
      "The in-sample MSE is 0.010\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.50%\n",
      "The out-of-sample MSE is 0.008\n",
      "CPU times: user 9.41 s, sys: 35 ms, total: 9.45 s\n",
      "Wall time: 1.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate(y_trn, XGB.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, XGB.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f8208d8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1830e2d4",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97852cb0",
   "metadata": {},
   "source": [
    "Next, we consider Neural Networks. As neural nets are highly parameterised, it is easy to overfit. I use the regularization methods mentioned in the paper, say _learning rate shrinkage_ (incorporated in Adam solver), _early stopping_, _batch normaliztion_ and _ensembles_. Besides, another requirement from so many parameters is more data. Therefore, if you use tiny dataset to have a taste of neural nets, it is very likely that it underperform simpler models. However, unlike the paper, my NN5 has best out-of-sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4e0a784f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 17:23:29.911495: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "from keras.regularizers import L1L2\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# customized metrics\n",
    "# out-of-sample r squared for keras\n",
    "def R_oos_tf(y_true, y_pred):\n",
    "    resid = tf.square(y_true-y_pred)\n",
    "    denom = tf.square(y_true)\n",
    "    return 1 - tf.divide(tf.reduce_mean(resid),tf.reduce_mean(denom))\n",
    "\n",
    "# data standardization\n",
    "# please standardize the data if BatchNormalization is not used\n",
    "def standardize(X_trn, X_vld, X_tst):\n",
    "    mu_trn = np.mean(np.array(X_trn),axis=0).reshape((1,X_trn.shape[1]))\n",
    "    sigma_trn = np.std(np.array(X_trn),axis=0).reshape((1,X_trn.shape[1]))\n",
    "\n",
    "    X_trn_std = (np.array(X_trn)-mu_trn)/sigma_trn\n",
    "    X_vld_std = (np.array(X_vld)-mu_trn)/sigma_trn\n",
    "    X_tst_std = (np.array(X_tst)-mu_trn)/sigma_trn\n",
    "    return X_trn_std, X_vld_std, X_tst_std\n",
    "\n",
    "# NN class\n",
    "class NN:\n",
    "    \n",
    "    def __init__(\n",
    "        self, n_layers=1, loss='mse', l1=1e-5, l2=0, learning_rate=.01, BatchNormalization=True, patience=5,\n",
    "        epochs=100, batch_size=3000, verbose=1, random_state=12308, monitor='val_R_oos_tf', base_neurons=5\n",
    "    ):\n",
    "        self.n_layers = n_layers\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.learning_rate = learning_rate\n",
    "        self.BatchNormalization = BatchNormalization\n",
    "        self.patience = patience\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        self.monitor = monitor\n",
    "        self.base_neurons = base_neurons\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param in params.keys():\n",
    "            setattr(self, param, params[param])\n",
    "        return self\n",
    "    \n",
    "    def fit(self, X_trn, y_trn, X_vld, y_vld):\n",
    "        # fix random seed for reproducibility\n",
    "        random.seed(self.random_state)\n",
    "        np.random.seed(self.random_state)\n",
    "        tf.random.set_seed(self.random_state)\n",
    "        \n",
    "        # model construction\n",
    "        mod = Sequential()\n",
    "        mod.add(Input(shape=(X_trn.shape[1],)))\n",
    "        \n",
    "        for i in np.arange(self.n_layers,0,-1):\n",
    "            if self.n_layers>self.base_neurons:\n",
    "                if self.n_layers == i:\n",
    "                    mod.add(Dense(2**i, activation='relu'))\n",
    "                else:\n",
    "                    mod.add(Dense(2**i, activation='relu', kernel_regularizer=L1L2(self.l1,self.l2)))\n",
    "            else:\n",
    "                if self.n_layers == i:\n",
    "                    mod.add(Dense(2**(self.base_neurons-(self.n_layers-i)), activation='relu'))\n",
    "                else:\n",
    "                    mod.add(Dense(2**(self.base_neurons-(self.n_layers-i)), \n",
    "                                  activation='relu', kernel_regularizer=L1L2(self.l1,self.l2)))\n",
    "            if self.BatchNormalization:\n",
    "                mod.add(BatchNormalization())\n",
    "        \n",
    "        mod.add(Dense(1, kernel_regularizer=L1L2(self.l1,self.l2)))\n",
    "        \n",
    "        # early stopping\n",
    "        earlystop = tf.keras.callbacks.EarlyStopping(monitor=self.monitor, patience=self.patience)\n",
    "\n",
    "        # Adam solver\n",
    "        opt = Adam(learning_rate=self.learning_rate)\n",
    "        \n",
    "        # compile the model\n",
    "        mod.compile(loss=self.loss,\n",
    "                    optimizer=opt,\n",
    "                    metrics=[R_oos_tf])\n",
    "\n",
    "        # fit the model\n",
    "        mod.fit(X_trn, np.array(y_trn).reshape((len(y_trn),1)), epochs=self.epochs, batch_size=self.batch_size, \n",
    "                callbacks=[earlystop], verbose=self.verbose, \n",
    "                validation_data=(X_vld,np.array(y_vld).reshape((len(y_vld),1))))\n",
    "        \n",
    "        self.model = mod\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X, verbose=self.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "196725d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 17:23:32.181889: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -1.03620%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -1120.98877%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -74.98328%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -159.95616%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -1.55934%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -1131.84176%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.34308%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -265.41539%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos 0.34% on validation set.\n",
      "############################################################\n",
      "CPU times: user 1min 40s, sys: 13.7 s, total: 1min 53s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN1-Regression-[32(relu)-1(linear)]\n",
    "\n",
    "params = {\n",
    "    'n_layers': [1],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN1 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8389f630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15924"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a14ac67a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is -34.16%\n",
      "The in-sample MSE is 0.015\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 3.53%\n",
      "The out-of-sample MSE is 0.010\n",
      "CPU times: user 4.95 s, sys: 1.04 s, total: 5.98 s\n",
      "Wall time: 4.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate(y_trn, NN1.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, NN1.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "72c098ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "99994a57",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -312.10330%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -177.64599%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -8.69084%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -441.51946%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -171.93004%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -805.10505%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 1.39125%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -103.10949%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos 1.39% on validation set.\n",
      "############################################################\n",
      "CPU times: user 2min 48s, sys: 18.3 s, total: 3min 6s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN2-Regression-[32(relu)-16(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [2],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN2 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a7aea326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3077"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aa3c9eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 6.39%\n",
      "The in-sample MSE is 0.011\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 2.14%\n",
      "The out-of-sample MSE is 0.008\n",
      "CPU times: user 5.15 s, sys: 1.01 s, total: 6.16 s\n",
      "Wall time: 4.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate(y_trn, NN2.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, NN2.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "de107b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0c52a69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -301.55560%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -273.78096%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.02979%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -26.13849%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -84.11816%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -248.25511%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.00314%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.58710%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos 0.59% on validation set.\n",
      "############################################################\n",
      "CPU times: user 2min 35s, sys: 14.6 s, total: 2min 50s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN3-Regression-[32(relu)-16(relu)-8(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [3],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN3 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dc7b4448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6804"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "599a367c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is -7.04%\n",
      "The in-sample MSE is 0.012\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is -8.72%\n",
      "The out-of-sample MSE is 0.009\n",
      "CPU times: user 5.08 s, sys: 793 ms, total: 5.87 s\n",
      "Wall time: 4.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate(y_trn, NN3.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, NN3.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "572be9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "862a0c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -35.57422%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -25.37831%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 1.02297%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -4.08690%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -12.69004%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -38.17034%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 1.59889%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.61755%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos 1.60% on validation set.\n",
      "############################################################\n",
      "CPU times: user 2min 37s, sys: 13.9 s, total: 2min 51s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN4-Regression-[32(relu)-16(relu)-8(relu)-4(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [4],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN4 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bc3f0a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14362"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e24e1772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is -12.20%\n",
      "The in-sample MSE is 0.013\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is -1.61%\n",
      "The out-of-sample MSE is 0.009\n",
      "CPU times: user 5.26 s, sys: 795 ms, total: 6.05 s\n",
      "Wall time: 4.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate(y_trn, NN4.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, NN4.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bc206184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "750bc124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -16.20821%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.89422%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 1.47867%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 1.71241%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -5.13381%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 1.82484%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -208.99782%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 2.83045%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos 2.83% on validation set.\n",
      "############################################################\n",
      "CPU times: user 3min 12s, sys: 15.9 s, total: 3min 28s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN5-Regression-[32(relu)-16(relu)-8(relu)-4(relu)-2(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [5],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN5 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4f2cd08e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16121"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b0fc5e73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is -2.50%\n",
      "The in-sample MSE is 0.012\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is -0.03%\n",
      "The out-of-sample MSE is 0.009\n",
      "CPU times: user 5.31 s, sys: 780 ms, total: 6.09 s\n",
      "Wall time: 4.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate(y_trn, NN5.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, NN5.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4cc98952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8277ca18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "50/50 [==============================] - 4s 70ms/step - loss: 43.9144 - R_oos_tf: -3655.5723 - val_loss: 5.3596 - val_R_oos_tf: -451.6760\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 1.7655 - R_oos_tf: -7.6937 - val_loss: 1.5363 - val_R_oos_tf: -14.2738\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 1.3056 - R_oos_tf: -0.5168 - val_loss: 1.1948 - val_R_oos_tf: -2.1730\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 1.0654 - R_oos_tf: -0.2869 - val_loss: 0.9585 - val_R_oos_tf: -1.9641\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 0.8354 - R_oos_tf: -0.3059 - val_loss: 0.8743 - val_R_oos_tf: -20.0934\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 3s 64ms/step - loss: 0.6238 - R_oos_tf: -0.2343 - val_loss: 0.7272 - val_R_oos_tf: -26.9446\n",
      "CPU times: user 1min 51s, sys: 1.36 s, total: 1min 52s\n",
      "Wall time: 19.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7dd04b4fa0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Wide-NN1-Regression-[1024(relu)-1(linear)]\n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "from keras.regularizers import L1L2\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "c = 10\n",
    "l1 = 1e-2\n",
    "l2 = 1e-2\n",
    "monitor = 'val_R_oos_tf'\n",
    "patience = 5\n",
    "learning_rate = 1e-1\n",
    "epochs = 100\n",
    "batch_size = int(X_trn.shape[0]/50)\n",
    "verbose = 1\n",
    "random_state = 12308\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "tf.random.set_seed(random_state)\n",
    "\n",
    "NN1W = Sequential()\n",
    "NN1W.add(Input(X_trn.shape[1]))\n",
    "NN1W.add(Dense(2**c, activation='relu'))\n",
    "NN1W.add(BatchNormalization())\n",
    "NN1W.add(Dense(1, kernel_regularizer=L1L2(l1,l2)))\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=patience)\n",
    "\n",
    "opt = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# compile the model\n",
    "NN1W.compile(loss='mse',\n",
    "             optimizer=opt,\n",
    "             metrics=[R_oos_tf])\n",
    "\n",
    "# fit the model\n",
    "NN1W.fit(X_trn, np.array(y_trn).reshape((len(y_trn),1)), epochs=epochs, batch_size=batch_size,\n",
    "         callbacks=[earlystop], \n",
    "         verbose=verbose,\n",
    "         validation_data=(X_vld,np.array(y_vld).reshape((len(y_vld),1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f2850083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1431"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fda9cc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is -1742.27%\n",
      "The in-sample MSE is 0.212\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is -55.03%\n",
      "The out-of-sample MSE is 0.013\n",
      "CPU times: user 15.7 s, sys: 612 ms, total: 16.3 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate(y_trn, NN1W.predict(X_trn,verbose=0), insample=True) \n",
    "evaluate(y_tst, NN1W.predict(X_tst,verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "40d17084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1284"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b65225fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 6, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -129.40485%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 6, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -37.60405%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 6, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.17058%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 6, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.02012%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 6, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -4620.50051%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 6, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -422.96879%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 6, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.84491%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 6, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -2.57341%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 6, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos 0.84% on validation set.\n",
      "############################################################\n",
      "CPU times: user 3min 16s, sys: 16.4 s, total: 3min 33s\n",
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN6-Regression-[64(relu)-32(relu)-16(relu)-8(relu)-4(relu)-2(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [6],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN6 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8412ba38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22386"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5da985e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 10.72%\n",
      "The in-sample MSE is 0.010\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 1.74%\n",
      "The out-of-sample MSE is 0.010\n",
      "CPU times: user 5.86 s, sys: 792 ms, total: 6.65 s\n",
      "Wall time: 4.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate(y_trn, NN6.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, NN6.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c3aad59b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d49de0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
