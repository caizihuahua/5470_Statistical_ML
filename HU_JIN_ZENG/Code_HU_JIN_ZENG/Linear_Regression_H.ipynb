{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tqdm\n",
    "pd.set_option(\"display.max_columns\",None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('merge_data.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,Lasso,Ridge,ElasticNet\n",
    "\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The recursive performance evaluation scheme\n",
    "train_score=[0]*30\n",
    "validation_score=[0]*30\n",
    "test_score=[0]*30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "Train data:1960-1978\n",
      "Valuation data:1978-1990\n",
      "Test data:1990-1991\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'scaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#Normalize\u001b[39;00m\n\u001b[1;32m     16\u001b[0m X_train\u001b[38;5;241m=\u001b[39mdata_train\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexcess_ret\u001b[39m\u001b[38;5;124m\"\u001b[39m,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 17\u001b[0m X_train\u001b[38;5;241m=\u001b[39m\u001b[43mscaler\u001b[49m\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\n\u001b[1;32m     18\u001b[0m X_validation\u001b[38;5;241m=\u001b[39mdata_validation\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexcess_ret\u001b[39m\u001b[38;5;124m\"\u001b[39m,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     19\u001b[0m X_validation\u001b[38;5;241m=\u001b[39mscaler\u001b[38;5;241m.\u001b[39mtransform(X_validation)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scaler' is not defined"
     ]
    }
   ],
   "source": [
    "#OLS+H\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "bestRSqr = float(\"-inf\")\n",
    "best_data1 = None\n",
    "bestOLS_H = None\n",
    "best_year = 0\n",
    "\n",
    "for i in range(0,30):\n",
    "    print('Batch {}:\\nTrain data:1960-{}\\nValuation data:{}-{}\\nTest data:{}-{}'.format(i,1978+i,1978+i,1990+i,1990+i,1991+i))\n",
    "    data_train=data2[(data2.yyyymm>=196001)&(data2.yyyymm<197801+i*100)]\n",
    "    data_validation=data2[(data2.yyyymm>=197801+i*100)&(data2.yyyymm<199001+i*100)]\n",
    "    data_test=data2[(data2.yyyymm>=196001+i*100)&(data2.yyyymm<196101+i*100)]\n",
    "\n",
    "    #Normalize\n",
    "    X_train=data_train.drop(\"excess_ret\",axis=1).copy()\n",
    "    X_train=scaler.fit_transform(X_train)\n",
    "    X_validation=data_validation.drop(\"excess_ret\",axis=1).copy()\n",
    "    X_validation=scaler.transform(X_validation)\n",
    "    X_test=data_test.drop(\"excess_ret\",axis=1).copy()\n",
    "    X_test=scaler.transform(X_test)\n",
    "\n",
    "    y_train=data_train[[\"excess_ret\"]].copy()\n",
    "    y_validation=data_validation[[\"excess_ret\"]].copy()\n",
    "    y_test=data_test[[\"excess_ret\"]].copy()\n",
    "    y_train=scaler.fit_transform(y_train)\n",
    "    y_test=scaler.transform(y_test)\n",
    "    y_validation=scaler.transform(y_validation)\n",
    "    \n",
    "    OLS_H = SGDRegressor(loss='huber',alpha=1e-3,epsilon=0.05,learning_rate='optimal')\n",
    "    OLS_H.fit(X_train, y_train)\n",
    "    y_train_pred=OLS_H.predict(X_train)\n",
    "    train_score[i]=r2_score(y_train,y_train_pred)\n",
    "    y_validation_pred=OLS_H.predict(X_validation)\n",
    "    validation_score[i]=r2_score(y_validation,y_validation_pred)\n",
    "    y_test_pred=OLS_H.predict(X_test)\n",
    "    test_score[i]=r2_score(y_test,y_test_pred)\n",
    "    currentRSqr = test_score[i]\n",
    "\n",
    "    if(currentRSqr>bestRSqr):\n",
    "        bestRSqr=currentRSqr\n",
    "        bestOLS_H = OLS_H\n",
    "        best_year = 196001+i*100\n",
    "        \n",
    "    del data_train,data_validation,data_test,X_train,y_train,X_test,y_test,X_validation,y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT RESULT\n",
    "test_score=pd.DataFrame(test_score)\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performence of OLS_H\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"OLS_H Performance\")\n",
    "year=np.arange(1990,2020)\n",
    "plt.plot(year,test_score.values)\n",
    "plt.xlabel(\"year\")\n",
    "plt.ylabel(\"score--R-square\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Importance \n",
    "#X_test0 = best_data1.drop(best_data1.columns[0],axis=1,inplace=True).copy()\n",
    "\n",
    "X_test0 = data2[(data2.yyyymm>=best_year)&(data2.yyyymm<best_year+100)]\n",
    "print(X_test0.shape)\n",
    "a = X_test0.drop(\"excess_ret\",axis=1).copy()\n",
    "print(a.shape)\n",
    "id = a.columns\n",
    "L = len(a.columns)\n",
    "train_score=[0]*L\n",
    "validation_score=[0]*L\n",
    "test_score1 = [0]*L\n",
    "importance_OLS_H=[0]*L\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable importance\n",
    "\n",
    "for i in range(0,L):\n",
    "    \n",
    "    X_test = X_test0.drop(\"excess_ret\",axis=1).copy()\n",
    "    X_test.loc[:,id[i]]=0\n",
    "    X_test=scaler.fit_transform(X_test)\n",
    "\n",
    "    y_test=X_test0[[\"excess_ret\"]].copy()\n",
    "    y_test=scaler.fit_transform(y_test)\n",
    "    \n",
    "\n",
    "    y_test_pred=bestOLS_H.predict(X_test)\n",
    "    test_score1[i] = r2_score(y_test,y_test_pred)\n",
    "    importance_OLS_H[i] = bestRSqr - test_score1[i]\n",
    "\n",
    "    \n",
    "    del X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT\n",
    "importance_OLS_H = np.abs(importance_OLS_H)\n",
    "importance_OLS_H1 = pd.DataFrame(importance_OLS_H,columns=['Importance'], index=id)\n",
    "importance_OLS_H1 = importance_OLS_H1.sort_values(by='Importance',ascending=True)\n",
    "importance_OLS_H1 = importance_OLS_H1.tail(20)\n",
    "importance_OLS_H1.plot(kind='barh', figsize=(9, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable importance BY COE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = data2.columns\n",
    "id = a.drop(\"excess_ret\").copy()\n",
    "\n",
    "importance_OLS_H = OLS_H.coef_\n",
    "importance_OLS_H = np.abs(importance_OLS_H)\n",
    "coefs_OLS_H = pd.DataFrame(importance_OLS_H,columns=['Importance'], index=id)\n",
    "coefs_OLS_H = coefs_OLS_H.sort_values(by='Importance',ascending=True)\n",
    "coefs_OLS_H= coefs_OLS_H.tail(20)\n",
    "coefs_OLS_H.plot(kind='barh', figsize=(9, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The recursive performance evaluation scheme\n",
    "train_score=[0]*30\n",
    "validation_score=[0]*30\n",
    "test_score=[0]*30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS3\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "bestRSqr = float(\"-inf\")\n",
    "best_data1 = None\n",
    "bestOLS_H3 = None\n",
    "best_year = 0\n",
    "\n",
    "for i in range(0,30):\n",
    "    print('Batch {}:\\nTrain data:1960-{}\\nValuation data:{}-{}\\nTest data:{}-{}'.format(i,1978+i,1978+i,1990+i,1990+i,1991+i))\n",
    "    data_train=data2[(data2.yyyymm>=196001)&(data2.yyyymm<197801+i*100)]\n",
    "    data_validation=data2[(data2.yyyymm>=197801+i*100)&(data2.yyyymm<199001+i*100)]\n",
    "    data_test=data2[(data2.yyyymm>=196001+i*100)&(data2.yyyymm<196101+i*100)]\n",
    "\n",
    "    #Normalize\n",
    "    X_train=data_train[['mom12m','bm','mvel1']].copy()\n",
    "    X_train=scaler.fit_transform(X_train)\n",
    "    X_validation=data_validation[['mom12m','bm','mvel1']].copy()\n",
    "    X_validation=scaler.transform(X_validation)\n",
    "    X_test=data_test[['mom12m','bm','mvel1']].copy()\n",
    "    X_test=scaler.transform(X_test)\n",
    "\n",
    "    y_train=data_train[[\"excess_ret\"]].copy()\n",
    "    y_validation=data_validation[[\"excess_ret\"]].copy()\n",
    "    y_test=data_test[[\"excess_ret\"]].copy()\n",
    "    y_train=scaler.fit_transform(y_train)\n",
    "    y_test=scaler.transform(y_test)\n",
    "    y_validation=scaler.transform(y_validation)\n",
    "    \n",
    "    OLS_H = SGDRegressor(loss='huber',alpha=1e-3,epsilon=0.05,learning_rate='optimal')\n",
    "    OLS_H.fit(X_train, y_train)\n",
    "    y_train_pred=OLS_H.predict(X_train)\n",
    "    train_score[i]=r2_score(y_train,y_train_pred)\n",
    "    y_validation_pred=OLS_H.predict(X_validation)\n",
    "    validation_score[i]=r2_score(y_validation,y_validation_pred)\n",
    "    y_test_pred=OLS_H.predict(X_test)\n",
    "    test_score[i]=r2_score(y_test,y_test_pred)\n",
    "    currentRSqr = test_score[i]\n",
    "\n",
    "    if(currentRSqr>bestRSqr):\n",
    "        bestRSqr=currentRSqr\n",
    "        bestOLS_H3 = OLS_H\n",
    "        best_year = 196001+i*100\n",
    "        \n",
    "    del data_train,data_validation,data_test,X_train,y_train,X_test,y_test,X_validation,y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULT\n",
    "test_score=pd.DataFrame(test_score)\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enet best parameter\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# store r2_score\n",
    "rList=[]\n",
    "combineList = []\n",
    "\n",
    "for i in range(30):\n",
    "    df_train = data2[(data2.yyyymm>=195703)&(data2.yyyymm<197503)]\n",
    "    df_validation = data2[(data2.yyyymm>=197503)&(data2.yyyymm<198703)]\n",
    "    df_test = data2[(data2.yyyymm>=198703)&(data2.yyyymm<201603)]\n",
    "\n",
    "    df_train = df_train.copy()\n",
    "    df_validation = df_validation.copy()\n",
    "    df_test = df_test.copy()\n",
    "\n",
    "    trainingstart = 195703\n",
    "    trainingend = 197503 + i*100\n",
    "    validend = trainingend + 1200\n",
    "    testend = validend + 100\n",
    "\n",
    "    trainingMask = (data2.yyyymm >= trainingstart) & (data2.yyyymm< trainingend)\n",
    "    trainingData = data2.loc[trainingMask]\n",
    "    \n",
    "    validationMask = (data2.yyyymm >= trainingend) & (data2.yyyymm< validend)\n",
    "    validationData = data2.loc[validationMask]\n",
    "    \n",
    "    testMask = (data2.yyyymm >= validend) & (data2.yyyymm < testend)\n",
    "    testData = data2.loc[testMask]\n",
    "    \n",
    "    \n",
    "    trainingData_y = trainingData['excess_ret']\n",
    "    trainingData_x = trainingData.drop(['excess_ret','permno', 'yyyymm'], axis=1)\n",
    "    \n",
    "    validationData_y = validationData['excess_ret']\n",
    "    validationData_x = validationData.drop(['excess_ret','permno', 'yyyymm'], axis=1)\n",
    "    \n",
    "    testData_y = testData['excess_ret']\n",
    "    testData_x = testData.drop(['excess_ret','permno', 'yyyymm'], axis =1)\n",
    "    \n",
    "    bestRSqr = float(\"-inf\")\n",
    "    bestEnet = None\n",
    "    bestCombine = None\n",
    "    \n",
    "    for alpha in [10e-04,10e-01]:\n",
    "        for l1 in [0,0.25,0.5,0.75,1]:\n",
    "            ENreg_H = SGDRegressor(loss='huber',penalty='elasticnet',alpha=alpha,  l1_ratio=l1, epsilon=0.05,max_iter=1e6,shuffle=False)\n",
    "            ENreg_H.fit(trainingData_x,trainingData_y)\n",
    "    \n",
    "            pred = ENreg_H.predict(validationData_x)\n",
    "            currentRSqr = r2_score(validationData_y.values, pred)\n",
    "            \n",
    "            if(currentRSqr>bestRSqr):\n",
    "                bestRSqr=currentRSqr\n",
    "                bestEnet=ENreg_H\n",
    "                bestCombine=(alpha,l1)\n",
    "        \n",
    "    resultPred = bestEnet.predict(testData_x)\n",
    "    resultRSqr = r2_score(testData_y.values, resultPred)\n",
    "    print('result R square for batch',i,': ', resultRSqr, 'best combine:', bestCombine)\n",
    "    rList.append(resultRSqr)\n",
    "    combineList.append(bestCombine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The recursive performance evaluation scheme\n",
    "train_score1=[0]*30\n",
    "validation_score1=[0]*30\n",
    "test_score1=[0]*30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enet\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "bestRSqr = float(\"-inf\")\n",
    "bestEnet = None\n",
    "best_year = 0\n",
    "\n",
    "for i in range(0,30):\n",
    "    print('Batch {}:\\nTrain data:1960-{}\\nValuation data:{}-{}\\nTest data:{}-{}'.format(i,1978+i,1978+i,1990+i,1990+i,1991+i))\n",
    "    data_train=data2[(data2.yyyymm>=196001)&(data2.yyyymm<197801+i*100)]\n",
    "    data_validation=data2[(data2.yyyymm>=197801+i*100)&(data2.yyyymm<199001+i*100)]\n",
    "    data_test=data2[(data2.yyyymm>=196001+i*100)&(data2.yyyymm<196101+i*100)]\n",
    "\n",
    "    #Normalize\n",
    "    X_train=data_train.drop(\"excess_ret\",axis=1).copy()\n",
    "    X_train=scaler.fit_transform(X_train)\n",
    "    X_validation=data_validation.drop(\"excess_ret\",axis=1).copy()\n",
    "    X_validation=scaler.transform(X_validation)\n",
    "    X_test=data_test.drop(\"excess_ret\",axis=1).copy()\n",
    "    X_test=scaler.transform(X_test)\n",
    "\n",
    "    y_train=data_train[[\"excess_ret\"]].copy()\n",
    "    y_validation=data_validation[[\"excess_ret\"]].copy()\n",
    "    y_test=data_test[[\"excess_ret\"]].copy()\n",
    "    y_train=scaler.fit_transform(y_train)\n",
    "    y_test=scaler.transform(y_test)\n",
    "    y_validation=scaler.transform(y_validation)\n",
    "    \n",
    "    ENreg_H = SGDRegressor(loss='huber',penalty='elasticnet',alpha=1e-3,l1_ratio=0.5, epsilon=0.05,max_iter=1e6,shuffle=False)\n",
    "    ENreg_H.fit(X_train,y_train)\n",
    "    y_train_pred=ENreg_H.predict(X_train)\n",
    "    train_score1[i]=r2_score(y_train,y_train_pred)\n",
    "    y_validation_pred=ENreg_H.predict(X_validation)\n",
    "    validation_score1[i]=r2_score(y_validation,y_validation_pred)\n",
    "    y_test_pred=ENreg_H.predict(X_test)\n",
    "    test_score1[i]=r2_score(y_test,y_test_pred)\n",
    "    currentRSqr = test_score1[i]\n",
    "\n",
    "    if(currentRSqr>bestRSqr):\n",
    "        bestRSqr=currentRSqr\n",
    "        bestEnet=ENreg_H\n",
    "        best_year = 196001+i*100\n",
    "    \n",
    "    del data_train,data_validation,data_test,X_train,y_train,X_test,y_test,X_validation,y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "test_score=pd.DataFrame(test_score1)\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performence of ENET-H\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"ENT+H Performance\")\n",
    "year=np.arange(1990,2020)\n",
    "plt.plot(year,test_score1)\n",
    "plt.xlabel(\"year\")\n",
    "plt.ylabel(\"score--R-square\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Importance \n",
    "#X_test0 = best_data1.drop(best_data1.columns[0],axis=1,inplace=True).copy()\n",
    "\n",
    "X_test0 = data2[(data2.yyyymm>=best_year)&(data2.yyyymm<best_year+100)]\n",
    "print(X_test0.shape)\n",
    "a = X_test0.drop(\"excess_ret\",axis=1).copy()\n",
    "print(a.shape)\n",
    "id = a.columns\n",
    "L = len(a.columns)\n",
    "train_score=[0]*L\n",
    "validation_score=[0]*L\n",
    "test_score1 = [0]*L\n",
    "importance_ENreg_H=[0]*L\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable importance\n",
    "\n",
    "for i in range(0,L):\n",
    "    \n",
    "    X_test = X_test0.drop(\"excess_ret\",axis=1).copy()\n",
    "    X_test.loc[:,id[i]]=0\n",
    "    X_test=scaler.fit_transform(X_test)\n",
    "\n",
    "    y_test=X_test0[[\"excess_ret\"]].copy()\n",
    "    y_test=scaler.fit_transform(y_test)\n",
    "    \n",
    "\n",
    "    y_test_pred=bestEnet.predict(X_test)\n",
    "    test_score1[i] = r2_score(y_test,y_test_pred)\n",
    "    importance_ENreg_H[i] = bestRSqr - test_score1[i]\n",
    "\n",
    "    \n",
    "    del X_test,y_test,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "importance_ENreg_H = np.abs(importance_ENreg_H)\n",
    "importance_ENreg_H1 = pd.DataFrame(importance_ENreg_H,columns=['Importance'], index=id)\n",
    "importance_ENreg_H1 = importance_ENreg_H1.sort_values(by='Importance',ascending=True)\n",
    "importance_ENreg_H1 = importance_ENreg_H1.tail(20)\n",
    "importance_ENreg_H1.plot(kind='barh', figsize=(9, 7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
